# 🎓 Q&A: 핵심 질문 리스트

이 문서는 RAG/LLM 시스템 구축 과정에서 자주 발생하는 질문과 기술 면접/학습 점검용 답변을 정리했습니다.

---

## 🏗️ PART 1~3: 인프라 및 데이터

### Q1. VectorDB는 RDB(SQL)와 무엇이 다른가요?
- **RDB**는 정확한 값(키워드, ID)을 매칭하는 데 특화되어 있고, **VectorDB**는 데이터의 **의미적 유사도(거리)**를 계산하는 데 특화되어 있습니다.
- 예를 들어, RDB에서 `SELECT * FROM docs WHERE content LIKE '%휴가%'`를 하면 "연차"라는 단어는 못 찾지만, VectorDB는 "휴가"와 "연차"의 벡터 거리가 가까우므로 찾아낼 수 있습니다.

### Q2. 임베딩(Embedding)이란 무엇이며 왜 필요한가요?
- 컴퓨터는 사람의 언어(한글/영어)를 이해하지 못합니다. 임베딩은 텍스트를 **실수형 벡터(숫자 리스트)**로 변환하여 컴퓨터가 계산할 수 있게 만드는 과정입니다.
- 의미가 비슷한 단어는 벡터 공간에서 가까운 위치에 좌표가 찍히게 됩니다.

### Q3. 청킹(Chunking) 사이즈는 어떻게 정해야 하나요?
- 정답은 없지만, 일반적으로 **500~1000자(Token 기준 256~512)** 정도가 적당합니다.
- 너무 작으면 문맥(Context)이 잘려서 이해하기 어렵고, 너무 크면 LLM의 Context Window를 많이 차지하고 검색 정밀도가 떨어집니다. 보통 **Overlap(중복 구간)**을 10~20% 두어 문맥 끊김을 방지합니다.

---

## 🧠 PART 4~5: RAG 및 에이전트

### Q4. RAG를 썼는데도 할루시네이션(거짓말)을 합니다. 왜 그런가요?
- **검색 실패**: 애초에 이상한 문서를 가져왔을 경우 (Garbage In).
- **LLM의 고집**: LLM이 검색된 문서보다 자기가 학습한 지식(Pre-trained knowledge)을 더 우선시할 경우.
- **해결책**: 프롬프트에 "반드시 주어진 문서에서만 답하라", "문서에 없으면 모른다고 하라"고 강력하게 지시하거나, **ReRanker**를 도입해 검색 품질을 높여야 합니다.

### Q5. Router/Agent는 언제 필요한가요?
- 사용자의 질문이 **복합적**일 때 필요합니다.
- 예: "지난달 A팀 매출(DB 조회)이랑 우리 회사 휴가 규정(문서 검색) 알려줘."
- 단순 RAG는 DB를 볼 수 없고, 단순 SQL 봇은 문서를 볼 수 없으므로, 질문 의도를 판단해 적절한 도구를 연결해주는 **Router**가 필수적입니다.

---

## 🔧 PART 6~7: 튜닝 및 최적화

### Q6. LangChain이 너무 복잡합니다. 꼭 써야 하나요?
- 필수는 아닙니다. 단순한 API 호출은 `requests`나 `openai` 패키지로 직접 짜는 게 더 빠를 수 있습니다.
- 하지만 문서 로더(PDF, Docx 등), 텍스트 분할기, VectorDB 연동, 메모리 관리 등 **공통 기능을 직접 구현하는 비용**이 크기 때문에, 복잡한 앱을 만들 때는 LangChain 같은 프레임워크가 장기적으로 유리합니다.

### Q7. ReRanker를 쓰면 속도가 느려지지 않나요?
- 네, 느려집니다. 1차 검색(Vector)은 0.01초 단위지만, ReRanker(Cross-Encoder)는 문서 하나하나를 모델에 통과시키므로 수십 배 느립니다.
- 따라서 **1차에서 50~100개를 빠르게 추리고(Recall)**, **2차에서 상위 5~10개만 정밀하게 재순위화(Precision)**하는 **2-Pass 전략**을 사용합니다.

### Q8. Hybrid Search(하이브리드 검색)는 무엇인가요?
- **Vector Search(의미)** + **Keyword Search(단어 매칭)**을 결합한 것입니다.
- "2024년" 같은 특정 연도나 "갤럭시 S24" 같은 특정 모델명은 키워드 검색이 정확하고, "배터리 오래 가는 폰 추천해줘" 같은 질문은 벡터 검색이 유리합니다. 이 둘의 장점을 합치는 것입니다.

---

## 🚀 PART 8: 운영

### Q9. 로컬 LLM을 서비스에 쓸 때 가장 큰 문제는 무엇인가요?
- **동시성(Concurrency)**입니다. ChatGPT API는 수천 명이 동시에 써도 되지만, 로컬 GPU/CPU는 한 번에 하나의 요청을 처리하는 데도 자원을 많이 씁니다.
- 사용자가 몰리면 대기 시간이 길어지므로, **vLLM** 같은 고성능 서빙 프레임워크를 쓰거나 큐(Queue) 시스템을 잘 설계해야 합니다.

### Q10. 데이터 보안 측면에서 로컬 RAG의 장점은?
- 사내의 민감한 데이터(연봉, 계약서 등)가 OpenAI나 Google 서버로 전송되지 않고, **물리적으로 차단된 사내 서버(On-Premise)** 안에서만 맴돌기 때문에 보안 규정이 엄격한 금융/공공/기업 환경에 적합합니다.
