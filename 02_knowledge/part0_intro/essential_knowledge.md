# PART 0. 이 책의 목표와 핵심 아키텍처

## 1. 📌 탄생 배경 (Background)
- **문제점**: 기존 LLM(ChatGPT 등)은 범용적인 지식은 뛰어나지만, **"폐쇄된 사내 데이터"**를 알지 못함.
- **기존 해결책의 한계**:
  - **Fine-tuning**: 비용이 비싸고, 데이터가 바뀔 때마다 재학습해야 하며, 보안 이슈가 있음.
  - **단순 프롬프트**: 입력 길이 제한(Token Limit)으로 방대한 사내 문서를 다 넣을 수 없음.
- **해법**: RAG(검색 증강 생성)와 MCP(Model Context Protocol)를 결합하여 **"싸고, 빠르고, 보안이 유지되는"** 사내 AI 비서를 구축하고자 함.

## 2. 🔑 개념 (Concept)
- **RAG (Retrieval-Augmented Generation)**: LLM이 답변을 생성하기 전에, 외부 데이터베이스(VectorDB)에서 관련 정보를 **검색(Retrieval)**하여 참고 자료로 제공하는 기술.
- **MCP (Model Context Protocol)**: LLM이 데이터베이스(SQL)나 API 같은 외부 도구를 표준화된 방식으로 연결하여 사용할 수 있게 하는 **연결 프로토콜**.

## 3. ⚙️ 역할 (Role)
- **사내 지식 베이스**: 흩어진 문서(PDF, Word, HWP)를 검색 가능한 지식으로 변환.
- **업무 자동화**: 정형 데이터(매출, 연차) 조회와 비정형 데이터(규정) 검색을 통합하여 직원의 반복 질문 해결.

## 4. 💡 도입 이유 (Reason)
- **데이터 보안**: 로컬 LLM(Ollama)을 사용하여 데이터가 외부(OpenAI 등)로 유출되지 않음.
- **최신성 유지**: 모델 재학습 없이 문서만 업데이트하면 즉시 AI가 새로운 내용을 알게 됨.
- **비용 효율성**: 값비싼 GPU 클러스터 없이 16GB RAM 수준의 일반 워크스테이션에서 구축 가능.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **할루시네이션 감소**: 근거 문서를 보고 답하므로 거짓말이 줄어듦.<br>- **보안**: 사내망 내부에서 완결적인 시스템 구축 가능.<br>- **확장성**: 문서뿐만 아니라 DB, API 등 다양한 소스 연결 가능. |
| **단점** | - **초기 구축 복잡성**: VectorDB, 임베딩, 검색 파이프라인 구축 필요.<br>- **검색 품질 의존도**: 검색이 실패하면 답변도 실패함 (Garbage In, Garbage Out). |

## 6. 🆚 차별점 (Differences)
- **vs Fine-tuning**: 
  - Fine-tuning은 "시험 공부"를 다시 시키는 것(지식 내재화).
  - RAG는 "오픈북 테스트"를 보게 하는 것(참고 자료 활용). 사내 업무에는 RAG가 훨씬 효율적.
- **vs ChatGPT Enterprise**:
  - ChatGPT는 데이터가 외부로 전송될 수 있다는 우려가 존재하지만, 이 아키텍처는 **100% 로컬(On-Premise)** 구동이 가능.

## 7. 🔗 관련 기술 (Related Tech)
- **Ollama / DeepSeek-R1**: 로컬 구동 가능한 고성능 오픈소스 LLM.
- **LangChain**: LLM과 검색기, 도구를 연결하는 오케스트레이션 프레임워크.
- **ChromaDB**: 문서 임베딩을 저장하는 경량 벡터 데이터베이스.
