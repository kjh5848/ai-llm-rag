# [심화 학습] RAG의 핵심, 청킹(Chunking) 정복하기

이 문서는 `예제1.md`의 3단계에서 다룬 RAG 구현 중 **청킹(Checkng)** 기술에 대해 깊이 있게 다룹니다.
데이터를 어떻게 자르느냐에 따라 AI의 성능이 얼마나 달라지는지 코드로 직접 비교해 봅니다.

---

## ⚖️ 비교: 청킹을 한 것 vs 안 한 것

| 구분 | 청킹 정교화 (작게 쪼갬) | 청킹 미사용 (통째로 넣음) |
| :--- | :--- | :--- |
| **검색 품질** | 질문과 딱 맞는 부분만 정확히 찾아냄 | 관련 없는 내용까지 섞여서 검색됨 |
| **AI 집중력** | 관련 정보가 농축되어 답변이 정확함 | 정보가 너무 많아 엉뚱한 소리를 함 (비용 상승) |
| **비유** | **포스트잇**에서 답 찾기 | **두꺼운 백과사전** 통째로 읽고 답 찾기 |

---

## 3-1단계: 청킹없이 통째로 넣기 (❌ 비권장)

문서를 쪼개지 않고 하나의 긴 텍스트로 처리합니다.
모든 규정이 한 덩어리로 묶여 있어, 인사 규정을 물어봐도 보안/복지 규정까지 딸려옵니다.

### 파일명: `step3_rag_no_chunking.py`

```python
from langchain_classic.chains import RetrievalQA
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate

# 1. 청킹 미적용: 모든 텍스트를 하나의 문자열로 합침 (통짜 데이터)
context_all = """
[인사규정] 신입사원은 입사 후 처음 3년 동안은 연차가 없습니다. 대신 매월 1회 '리프레시 데이'를 유급으로 사용합니다.
[보안규정] 모든 직원은 사내에서 승인된 보안 USB만 사용해야 합니다.
[복지규정] 점심 식대는 무제한 법인카드를 지원합니다.
"""

# 하나의 거대한 문서로 만듦 -> 검색이 비효율적임
docs_bad = [Document(page_content=context_all, metadata={"source": "전체규정"})]

# 2. VectorDB 생성
print("문서를 학습(임베딩) 중입니다... (청킹 미적용)")
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma.from_documents(documents=docs_bad, embedding=embeddings)

# 3. 검색기 및 프롬프트 설정
# 통째로 하나뿐이므로 k=1로 검색해도 전체가 다 나옵니다.
retriever = vectorstore.as_retriever(search_kwargs={"k": 1}) 

template = """참고 정보를 바탕으로 질문에 한국어로 답하세요.
참고 정보: {context}
질문: {question}
답변:"""
PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

# 4. RAG 체인 실행
llm = ChatOllama(model="deepseek-r1:8b", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, 
    retriever=retriever, 
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)

question = "신입사원 휴가 규정 알려줘."
result = qa_chain.invoke({"query": question})
print(f"\nAI 답변:\n{result['result']}")
```

---

## 3-2단계: 청킹으로 쪼개서 넣기 (✅ 권장)

문서를 의미 단위(문단 등)로 쪼개어 리스트에 담습니다.
검색엔진이 질문과 가장 관련 있는 '파편'만 찾아냅니다.

### 파일명: `step3_rag_chunking.py`

```python
from langchain_classic.chains import RetrievalQA
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate

# 1. 청킹(Chunking) 적용: 문서를 문맥 단위로 쪼개서 리스트에 담음
docs = [
    Document(page_content="[인사규정] 신입사원은 입사 후 처음 3년 동안은 연차가 없습니다. 대신 매월 1회 '리프레시 데이'를 유급으로 사용합니다.", metadata={"source": "인사규정"}),
    Document(page_content="[보안규정] 모든 직원은 사내에서 승인된 보안 USB만 사용해야 합니다.", metadata={"source": "보안규정"}),
    Document(page_content="[복지규정] 점심 식대는 무제한 법인카드를 지원합니다.", metadata={"source": "복지규정"}),
]

# 2. VectorDB 생성
print("문서를 학습(임베딩) 중입니다... (청킹 적용)")
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)

# 3. 검색기 및 프롬프트 설정
# 질문과 가장 관련 있는 상위 2개의 조각만 가져옵니다.
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

template = """참고 정보를 바탕으로 질문에 한국어로 답하세요.
참고 정보: {context}
질문: {question}
답변:"""
PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

# 4. RAG 체인 실행
llm = ChatOllama(model="deepseek-r1:8b", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, 
    retriever=retriever, 
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)

question = "신입사원 휴가 규정 알려줘."
result = qa_chain.invoke({"query": question})
print(f"\nAI 답변:\n{result['result']}")
```

---

## ✅ 결론: 왜 실무에서는 청킹이 필수인가요?

1. **비용 절감**: LLM은 입력 토큰(글자 수) 단위로 과금됩니다. 불필요한 전체 문서를 넣으면 돈을 버리는 셈입니다.
2. **정확도 향상**: "보안 규정"을 물었는데 "식대 규정"까지 주면 AI가 혼란스러워(Hallucination) 합니다.
3. **속도 향상**: 처리할 데이터가 적을수록 답변 속도가 빨라집니다.

따라서 실무에서는 `RecursiveCharacterTextSplitter` 같은 도구를 사용하여 수백 페이지의 PDF를 자동으로 청킹하여 DB에 넣습니다.
