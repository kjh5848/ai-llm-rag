# 6장. 지식 저장소 구축: 임베딩과 벡터DB 실습

5장에서 우리는 문서를 깨끗한 **텍스트** 로 만들었습니다. 하지만 컴퓨터는 텍스트의 의미를 바로 이해하지 못합니다. 본 장에서는 텍스트를 **숫자(벡터)** 로 변환하여 AI가 검색할 수 있는 **지식 저장소(VectorDB)** 를 구축합니다.

---

## 1. 텍스트를 숫자로: 임베딩(Embedding)

임베딩은 단어의 의미를 다차원 공간의 좌표로 변환하는 기술입니다.

![임베딩 개념도](./images/06장_embedding_concept.png)

위 그림처럼 "사과"와 "배"는 가깝게, "자동차"는 멀게 배치됩니다. 이를 통해 AI는 단순 키워드 매칭이 아니라 **의미적 유사성(Semantic Similarity)** 을 판단할 수 있습니다.

---

## 2. [실습] 지식 적재(Ingestion) 파이프라인

이미 3장에서 클론한 `ai-llm-rag-study` 저장소 내의 실습 폴더로 이동합니다.

### 1단계: 실습 폴더로 이동

실습 폴더인 `06_vector_db` 로 이동합니다.

### 2단계: 의존성 설치

`chromadb` 와 `langchain-chroma` 등을 설치합니다.

```bash
pip install -r requirements.txt
```

### 3단계: 청킹(Chunking) 확인

AI 모델은 한 번에 읽을 수 있는 글자 수에 제한이 있습니다. 따라서 긴 문서를 의미 단위로 잘라야 합니다.

`chunking.py` 파일을 확인합니다. 이 코드는 문서를 500자 단위로 자르고, 문맥이 끊기지 않도록 50자를 겹치게(Overlap) 설정했습니다.

```bash
python chunking.py
```

> **Tip**: `chunking.py`는 긴 문서를 AI가 처리하기 좋은 크기(Chunk)로 잘라내는 스크립트입니다. `chunk_size`를 조절하여 자르는 단위를 변경할 수 있습니다.

- **실행 결과**: 문서가 여러 개의 `Document` 객체로 분할된 것을 확인할 수 있습니다.

### 4단계: 벡터DB 저장 (Ingest)

이제 잘라낸 텍스트를 벡터로 변환하여 **ChromaDB** 에 저장합니다. 이때 2장에서 설치한 `nomic-embed-text` 모델을 사용합니다.

`ingest.py` 파일을 확인하고 실행합니다.

```bash
python ingest.py
```

> **Tip**: `ingest.py`는 잘라낸 텍스트 청크를 임베딩 모델을 통해 벡터로 변환하고, 이를 ChromaDB에 영구 저장하는 스크립트입니다.

- **실행 결과**:
  터미널에 "Stored 15 chunks to ChromaDB" 같은 메시지가 뜨며, 폴더 내에 `chroma_db/` 라는 디렉토리가 생성됩니다. 이것이 바로 우리만의 **지식 저장소** 입니다.

---

## 3. [실습] 잘 저장되었나? 검색 테스트

저장이 잘 되었는지 확인하기 위해 질문을 던져봅니다.

### 1단계: 검색 실행

`query.py` 파일을 확인합니다. "연차 규정 알려줘"라는 질문과 가장 유사한 문서를 DB에서 찾아옵니다.

```bash
python query.py
```

> **Tip**: `query.py`는 사용자의 질문을 벡터로 변환한 뒤, DB에 저장된 문서 중 코사인 유사도가 가장 높은 문서를 찾아오는 검색 테스트 스크립트입니다.

- **실행 결과**:

```text
질문: 신입사원 연차 규정
--------------------------------------------------
검색된 문서 1: [인사규정] 신입사원은 입사 후 3년 동안...
유사도 점수: 0.85
--------------------------------------------------
```

질문과 가장 관련 높은 문서를 정확히 찾아냈다면 성공입니다.

---

## 4. 로컬 벡터DB의 장점

우리는 클라우드(Pinecone 등)를 쓰지 않고 **로컬 ChromaDB** 를 구축했습니다.

- **보안**: 사내 데이터가 외부로 나가지 않습니다.
- **비용**: 100% 무료이며, API 비용이 들지 않습니다.
- **속도**: 네트워크 대기 시간이 없어 검색이 매우 빠릅니다.

이제 우리 시스템은 수만 페이지의 문서 중 질문과 가장 관련 있는 내용을 밀리초(ms) 단위로 찾아낼 수 있게 되었습니다. 다음 장(7장)에서는 이 지식 창고를 활용하여 실제 답변을 생성하는 **RAG 엔진** 을 완성해 보겠습니다.
