# 📘 RAG/LLM 시스템 구축을 위한 기술 상세 레퍼런스 가이드북

이 문서는 실제 책 집필을 돕기 위한 **전문 기술 가이드북**입니다. 각 파트는 정보의 기술적 기원, 아키텍처 설계, 비즈니스 전략 등을 동일한 구조(Framework)로 분석하여 작가님이 어떤 챕터를 쓰더라도 일관된 기술적 깊이를 유지할 수 있도록 설계되었습니다.

> ⚠️ **주의**: 본 문서는 집필용 레퍼런스이며, 실제 책의 챕터 구성과 목차는 독자의 흥미와 스토리텔링에 맞춰 자유롭게 재구성하시기 바랍니다.

---

## 🗺️ 전체 가이드 맵 (Table of Contents)

### [PART 0. 전략 및 기초]
- [0. 이 책의 목표와 핵심 아키텍처](#파트-0-이-책의-목표와-핵심-아키텍처-기술-상세-레퍼런스)
- [0.5 개발 환경 설정 및 최적화](#파트-05-개발-환경-설정-기술-상세-레퍼런스)

### [PART 1~4. 핵심 RAG 엔진 구축]
- [1. FastAPI 백엔드 연동](#파트-1-fastapi로-사내-시스템-구축-기술-상세-레퍼런스)
- [2. 미가공 문서 수집 및 정제 전략](#파트-2-사내-문서-수집-및-정제-전략-기술-상세-레퍼런스)
- [3. VectorDB 기술 및 정보 수치화](#파트-3-vectordb-구축-지식의-수치화와-고속-검색-레퍼런스)
- [4. RAG 엔진 통합 및 오케스트레이션](#파트-4-rag-시스템-구축-검색-증강-생성의-통합-아키텍처-레퍼런스)

### [PART 5~8. 고도화 및 실제 운영]
- [5. 정형/비정형 통합 에이전트 설계](#파트-5-통합-에이전트-설계-정형비정형-데이터-결합-레퍼런스)
- [6. LangChain 기반 워크플로우 제어](#파트-6-langchain-오케스트레이션-시스템-연결-및-워크플로우-제어-레퍼런스)
- [7. 검색 품질 튜닝 및 최적화 기법](#파트-7-rag-튜닝-및-고도화-답변-품질의-한계를-극복하는-기술-레퍼런스)
- [8. Docker 기반의 상용 배포와 관리](#파트-8-배포와-운영-안정적인-ai-서비스-공급-전략-레퍼런스)

---

# 파트 0. 이 책의 목표와 핵심 아키텍처: 기술 상세 레퍼런스

이 문서는 실무 중심의 AI 시스템 구축을 위한 핵심 기술 배경과 아키텍처 설계를 다룹니다. 작가님이 책의 챕터를 구성할 때 논리적 근거로 활용할 수 있도록 전문적인 기술 분석을 제공합니다.

---

## 0.1 기술적 기원: 왜 이 아키텍처가 필요한가?
*   **LLM의 정보 격리성**: 범용 LLM은 학습 데이터 컷오프(Cut-off) 이후의 정보나 기업 내부의 비공개 데이터(PDF, DB)를 물리적으로 알 수 없습니다.
*   **데이터 휘발성 대응**: 규정 변경, 인사 이동 등 매일 변하는 비즈니스의 동적 데이터를 정적인 모델 가중치(Fine-tuning)에 담는 것은 유지보수 측면에서 불가능에 가깝습니다.
*   **보안과 지식 주권**: 기업의 핵심 자산인 문서를 외부 클라우드로 전송하지 않고도 지능형 서비스를 구현하고자 하는 수요가 급증함에 따라 'On-Premise AI' 아키텍처가 필수적이 되었습니다.

## 0.2 아키텍처 설계: 핵심 구조와 기술적 정의
*   **RAG (Retrieval-Augmented Generation)**: 생성 모델이 답변하기 전, 신뢰할 수 있는 외부 데이터소스에서 지식을 **검색(Retrieval)**하여 컨텍스트를 **보강(Augmentation)**한 뒤 답변을 **생성(Generation)**하는 기술적 프레임워크입니다.
*   **MCP (Model Context Protocol)**: AI 모델이 사내 시스템(SQL, API, 로컬 파일 서버)과 표준화된 방식으로 소통하기 위한 연결 규격입니다. RAG가 '읽기' 중심이라면 MCP는 '행동과 연동' 중심의 프로토콜입니다.
*   **Reasoning (추론 능력)**: 단순히 단어를 나열하는 것이 아니라, 검색된 정보들 사이의 충돌을 해석하고 복잡한 계산 과정을 단계별(Chain of Thought)로 수행하는 능력입니다.

## 0.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **지식 엔진 (Knowledge Engine)**: 비정형 문서를 데이터화하여 AI의 장기 기억 장치로 변환합니다.
*   **의사결정 보조 (Decision Support)**: "규정상 이 결재가 가능한가?"와 같은 질문에 대해 규정 문서와 결재 DB를 실시간으로 대조하여 답변합니다.
*   **환각 방지 안전장치 (Anti-Hallucination)**: LLM이 모르는 내용을 지어내기보다, 주어진 문서 내에서만 사고하도록 논리적 제약을 겁니다.

## 0.4 비즈니스 전략: 도입이 필요한 실무적 근거
*   **비용 효율성 (Cost-Efficiency)**: 대규모 모델 재학습 없이 문서 업데이트만으로 시스템 사양을 유지할 수 있습니다.
*   **신뢰성 (Reliability)**: 답변과 함께 근거 문서의 출처(파일명, 페이지)를 제시할 수 있어 사용자가 AI를 믿고 업무에 활용할 수 있게 합니다.
*   **인프라 유연성**: 로컬 환경(Ollama)을 지원하므로 폐쇄망 내부 배포가 용이합니다.

## 0.5 트레이드오프: 기술적 강점과 시스템 제약 사항
*   **강점**
    *   **투명성**: AI가 왜 그렇게 답했는지 추적이 가능합니다.
    *   **유연성**: 파일 업로드만으로 지식 업데이트가 즉시 이루어집니다.
*   **단점**
    *   **검색 종속성**: 관련 문서를 찾지 못하면 LLM의 성능과 무관하게 오답을 냅니다.
    *   **설계 복합성**: 문서를 잘게 자르는 전략(Chunking)에 따라 품질이 크게 좌우됩니다.

## 0.6 비교 우위: 대안 기술과의 결정적 차이점
*   **vs Fine-tuning**: 
    *   Fine-tuning은 모델의 '말투'나 '특정 분야 전문성'을 높이지만, RAG는 '정확한 사실 정보'를 전달합니다.
    *   실무에서는 RAG로 지식을 공급하고, Fine-tuning으로 답변 형식을 다듬는 것이 최상의 조합입니다.
*   **vs SQL Chatbot**: 단순 SQL 챗봇은 문서(규정)를 이해하지 못하고, RAG만으로는 숫자 계산이 약합니다. 본 아키텍처는 이 둘을 통합한 하이브리드 지능을 지향합니다.

## 0.7 연관 생태계: 통합 기술 스택
*   **Vector Database (Chroma, Pinecone)**: 의미 기반 검색을 위한 고유 DB.
*   **Embedding Models (nomic-embed-text)**: 텍스트를 숫자로 변환하는 핵심 모델.
*   **Orchestration (LangChain, LlamaIndex)**: 전체 워크플로우를 제어하는 프레임워크.

---
**실습 연결 포인트**:
- 해당 아키텍처의 철학은 `01_basic_rag/tutorial_step1_fail.md`의 실패 사례 분석과 `step3_rag.py`의 구현 코드에서 확인하실 수 있습니다.


---

# 파트 0.5. 개발 환경 설정: 기술 상세 레퍼런스

이 문서는 실무에서 RAG 시스템을 구축하기 위해 필요한 기술 스택의 선정 이유와 환경 구성의 원리를 다룹니다. 작가님이 설치 과정을 설명할 때 각 도구의 기술적 위치를 명확히 할 수 있도록 돕습니다.

---

## 0.5.1 기술적 기원: 왜 인프라 격리와 사양 최적화가 필요한가?
*   **AI 환경의 하드웨어 민주화**: 과거 거대 모델 구동에는 전용 가속기(H100 등)가 필수였으나, 최근 양자화(Quantization) 기술의 발전으로 일반 PC급 하드웨어에서도 고성능 모델 구동이 가능해졌습니다.
*   **환경 격리의 필요성**: AI 라이브러리(PyTorch, LangChain 등)는 버전 충돌에 매우 민감합니다. 시스템 전체의 안정성을 위해 격리된 실행 환경이 필수적으로 요구됩니다.

## 0.5.2 아키텍처 설계: 인프라 구성의 핵심 정의
*   **Quantization (양자화)**: 모델의 가중치를 정밀도가 낮은 데이터 타입(예: 16bit → 4bit)으로 변환하여, 성능 손실을 최소화하면서 메모리 사용량과 연산 속도를 획기적으로 개선하는 기술입니다.
*   **Containerization (컨테이너화)**: 데이터베이스(PostgreSQL, Chroma) 등 인프라 요소를 운영체제와 분리하여 어느 환경에서든 동일하게 구동되도록 패키징하는 방식입니다.
*   **Virtual Environment (가상환경)**: 프로젝트별로 독립된 Python 실행 경로와 패키지 저장소를 확보하는 기술입니다.

## 0.5.3 작동 메커니즘: 각 도구의 시스템 내 역할
*   **모델 서빙 (Ollama)**: 모델 파일을 메모리에 로드하고, 외부에서 API(HTTP/JSON)를 통해 LLM 기능을 호출할 수 있게 해주는 엔진 역할을 수행합니다.
*   **데이터 거점 (PostgreSQL & Chroma)**: 정형 데이터(SQL)와 비정형 벡터 데이터(Vector)가 각각 보관되는 물리적 저장소입니다.
*   **의존성 관리 (pip)**: 시스템에 설치된 수십 개의 라이브러리가 상호 작용할 수 있도록 규격화된 설치와 관리를 담당합니다.

## 0.5.4 비즈니스 전략: 로컬 환경 구축의 실무적 근거
*   **로컬 실행**: 데이터 보안 준수와 API 비용 절감을 위해 모든 연산을 로컬 자원에서 처리하도록 구성했습니다.
*   **Docker 활용**: DB 설치 과정의 복잡성을 제거하고, 환경 오염 없이 깔끔한 설치/삭제를 지원하기 위해 도입되었습니다.
*   **Python 3.10+**: 최신 LangChain 라이브러리와 비동기 처리(Asyncio)의 안정적인 지원을 위해 선정되었습니다.

## 0.5.5 트레이드오프: 로컬 인프라의 강점과 제약 사항
*   **장점**
    *   **단일 제어**: 모델부터 DB까지 내 통제 하에 운영됩니다.
    *   **비용 예측 가능**: 추가적인 API 요금 없이 전기료와 하드웨어 감가상각만으로 운영이 가능합니다.
*   **단점**
    *   **메모리 압박**: RAM 16GB 미만 환경에서는 여러 모델을 동시에 띄우는 것이 어렵습니다.
    *   **설정 오버헤드**: 초기 인프라 구축에 수동적인 노력이 수반됩니다.

## 0.5.6 비교 우위: 클라우드 API와의 결정적 차별점
*   **로컬 vs 클라우드 API (OpenAI 등)**:
    *   클라우드는 초기 설정이 0에 가깝지만, 기업 내부 폐쇄망에서는 사용이 불가능하다는 명확한 한계가 있습니다.
    *   본 환경 구성은 하드웨어 사양이 허용하는 한 최고의 보안 수준을 제공합니다.
*   **Ollama vs Raw Llama.cpp**:
    *   Ollama는 복잡한 컴파일 과정이나 파라미터 설정을 자동화하여, 개발자가 인프라보다 로직에 집중할 수 있게 해줍니다.

## 0.5.7 연관 생태계: 하드웨어 및 소프트웨어 통합 스택
*   **Docker Desktop**: 컨테이너 관리 GUI.
*   **Python venv**: 표준 가상환경 라이브러리.
*   **NVIDIA CUDA / Apple Metal**: GPU 가속을 위해 모델이 사용하는 연산 라이브러리.

---
**실습 연결 포인트**:
- 설치된 환경의 정상 작동 여부는 `01_basic_rag/install_guide.md`의 테스트 단계에서 검증할 수 있습니다.


---

# 파트 1. FastAPI로 사내 시스템 구축: 기술 상세 레퍼런스

이 문서는 AI 에이전트가 접근할 수 있는 '데이터 원천(Source of Truth)'이 되는 사내 백엔드 시스템 구축의 기술적 원리와 설계 방식을 다룹니다.

---

## 1.1 기술적 기원: 왜 AI 에이전트를 위한 전용 백엔드가 필요한가?
*   **AI와 데이터의 가교**: 뇌(LLM)가 근육(실행)과 장기(데이터)를 통제하기 위해서는 표준화된 신경망(API)이 필요합니다.
*   **비즈니스 로직의 통합**: 단순 LLM 호출을 넘어, 기업 특유의 비즈니스 규칙(결재 라인, 권한 등)을 반영한 안전한 데이터 제공 계층이 필요합니다.

## 1.2 아키텍처 설계: 비동기 백엔드의 핵심 정의
*   **ASGI (Asynchronous Server Gateway Interface)**: Python의 비동기 처리를 지원하는 표준 인터페이스로, 수천 개의 AI 스트리밍 요청을 효율적으로 처리할 수 있게 합니다.
*   **Pydantic**: 데이터 검증 및 설정을 위한 라이브러리로, API 입출력 값의 타입 안정성을 보장하고 자동 문서화의 근간이 됩니다.
*   **DI (Dependency Injection)**: 데이터베이스 세션이나 모델 인스턴스를 필요한 곳에 주입하여 코드의 결합도를 낮추고 테스트 용이성을 높이는 설계 패턴입니다.

## 1.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **데이터 거버넌스**: AI가 날것의 DB에 직접 접근하지 않고, API라는 정제된 창구를 통해 허가된 데이터만 조회하도록 통제합니다.
*   **RESTful 인터페이스**: HTTP 메서드(GET, POST 등)를 통해 AI 에이전트가 이해하기 쉬운 규격화된 소통 방식을 제공합니다.
*   **자동화된 문서화 (OpenAPI/Swagger)**: 프론트엔드 개발자나 AI 에이전트(Tool Calling 시)가 API 사양을 즉시 파악할 수 있는 대화형 문서를 생성합니다.

## 1.4 비즈니스 전략: FastAPI 도입의 실무적 근거
*   **성능 최적화**: Python 웹 프레임워크 중 가장 빠른 성질을 가지며, 특히 입출력 대기 시간이 긴 AI 서비스에 최적화되어 있습니다.
*   **개발 생산성**: 현대적인 Python 문법(Type Hinting)을 적극 활용하여 버그를 줄이고 개발 속도를 높입니다.
*   **간결한 프론트엔드 연동**: 실습 환경에서는 Jinja2 템플릿 엔진을 사용하여 별도의 복잡한 빌드 과정 없이 직관적인 관리자 UI를 구축할 수 있습니다.

## 1.5 트레이드오프: 백엔드 프레임워크의 강점과 제약 사항
*   **장점**
    *   **강력한 유효성 검사**: 잘못된 형식의 데이터가 시스템에 유입되는 것을 원천 차단합니다.
    *   **비동기 지원**: LLM의 긴 응답 시간 동안 서버의 다른 자원이 차단되지(Blocking) 않습니다.
*   **단점**
    *   **비동기 학습 곡선**: `async/await` 개념에 대한 깊은 이해가 필요합니다.
    *   **라이브러리 호환성**: 일부 오래된 SQL 라이브러리가 비동기를 완벽히 지원하지 않을 수 있습니다.

## 1.6 비교 우위: 대안 프레임워크와의 결정적 차이점
*   **vs Django**: Django는 무거운 '풀스택' 지향이나, AI 에이전트용 마이크로 API 서버로는 가볍고 빠른 FastAPI가 더 적합합니다.
*   **vs Flask**: Flask는 동기 방식 위주이며 대규모 비동기 처리에 한계가 있는 반면, FastAPI는 처음부터 고성능 비동기를 위해 설계되었습니다.

## 1.7 연관 생태계: 백엔드 및 DB 통합 스택
*   **SQLAlchemy**: Python 객체와 관계형 DB를 매핑하는 ORM 표준.
*   **Uvicorn**: FastAPI 앱을 실행하는 고성능 ASGI 서버.
*   **Jinja2**: 서버측 HTML 렌더링을 위한 템플릿 엔진.

---
**실습 연결 포인트**:
- 기본적인 API 설계와 데이터 CRUD 로직은 `01_basic_rag/tutorial_step1_fail.md`의 시스템 배경 설명 및 해당 단계의 백엔드 코드에서 확인할 수 있습니다.


---

# 파트 2. 사내 문서 수집 및 정제 전략: 기술 상세 레퍼런스

이 문서는 RAG 시스템의 성능을 결정짓는 가장 중요한 요소인 '데이터 품질 전문 관리'와 '비정형 데이터의 구조화' 전략을 다룹니다.

---

## 2.1 기술적 기원: 왜 데이터 전처리가 RAG 성능의 80%를 결정하는가?
*   **GIGO (Garbage In, Garbage Out)**: LLM의 추론 능력이 아무리 뛰어나도, 입력되는 문서 조각(Context)이 부정확하거나 전처리가 부실하면 결과물도 무의미해집니다.
*   **비정형 데이터의 복잡성**: 사내 문서는 PDF, Word, Excel 등 포맷이 제각각이며, 표(Table), 이미지, 계층적 구조(Heading) 등이 섞여 있어 기계적 읽기가 매우 어렵습니다.

## 2.2 아키텍처 설계: 비정형 데이터 정제 절차의 핵심 정의
*   **Parsing (파싱)**: 원본 파일(바이너리)에서 유의미한 텍스트와 레이아웃 정보를 추출하는 과정입니다.
*   **Chunking (청킹)**: 방대한 문서를 LLM이 한 번에 처리할 수 있는 최적의 크기로 분할하는 작업입니다.
*   **Context Window**: 모델이 한 번에 '볼 수 있는' 토큰의 한계치로, 청킹 사이즈 결정의 물리적 기준이 됩니다.

## 2.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **지식의 구조화**: 줄글 형태의 텍스트를 제목-본문-메타데이터 형태의 '검색 가능한 단위'로 재구성합니다.
*   **메타데이터 인젝션**: 단순히 텍스트만 저장하는 것이 아니라 생성일, 저자, 부서, 보안 등급 등의 정보를 꼬리표로 붙여 검색의 정밀도를 높입니다.
*   **노이즈 제거**: 불필요한 특수문자, 중복 공백, OCR 오류 등을 정제하여 임베딩 모델의 정확도를 개선합니다.

## 2.4 비즈니스 전략: 고퀄리티 데이터 파이프라인 구축의 실무적 근거
*   **검색 성능 최적화**: 적절한 크기로 잘린 문서는 벡터 공간에서 더 뚜렷한 의미 좌표를 갖게 됩니다.
*   **토큰 비용 관리**: 검색된 정보가 너무 길면 LLM 호출 비용(혹은 로컬 연산 시간)이 증가하므로 효율적인 압축이 필요합니다.
*   **다양한 포맷 지원**: 회사 내 산재한 HWP, PDF, Excel 데이터를 하나의 통합된 데이터 스트림으로 변환하기 위해 표준화된 파이브라인이 필요합니다.

## 2.5 트레이드오프: 데이터 정제 방식의 강점과 제약 사항
*   **장점**
    *   **정확도 향상**: 문서의 계층 구조를 보존하며 잘랐을 때 검색 가독성이 획기적으로 좋아집니다.
    *   **관리 효율**: 메타데이터를 활용하여 실시간 권한 제어(ACL)가 가능해집니다.
*   **단점**
    *   **전처리 복잡도**: 표(Table)나 수식이 많은 문서는 고도의 파이싱 기술(Layout-aware parsing)이 요구됩니다.
    *   **정보 손실 위험**: 청킹 과정에서 앞뒤 문맥이 끊기면 의미가 왜곡될 수 있습니다.

## 2.6 비교 우위: 대안 처리 기법과의 결정적 차이점
*   **단순 텍스트 추출 vs 구조적 파싱**: 
    *   단순 추출은 문서의 흐름을 파괴하지만, 구조적 파싱(Markdown 변환 등)은 제목과 본문의 관계를 유지하여 AI의 이해 돕습니다.
*   **고정 크기 청킹 vs 의미 단위 청킹**:
    *   고정 크기(Fixed-size)는 구현이 쉽지만 문장을 무작위로 자르는 반면, 의미 단위(Recursive character) 청킹은 문맥의 마침표나 단락을 기준으로 유연하게 작동합니다.

## 2.7 연관 생태계: 텍스트 추출 및 처리 통합 스택
*   **PyPDF / pypdf**: PDF 텍스트 추출의 표준 라이브러리.
*   **python-docx / openpyxl**: 워드 및 엑셀 정형화 도구.
*   **RecursiveCharacterTextSplitter**: 문맥을 최대한 보존하며 문서를 자르는 알고리즘.

---
**실습 연결 포인트**:
- 문서 수집 및 표준화 실습은 `01_basic_rag/tutorial_step2_context.md`와 해당 단계에서 문서를 로드하고 리스트를 만드는 과정을 통해 직접 확인하실 수 있습니다.


---

# 파트 3. VectorDB 구축: 지식의 수치화와 고속 검색 레퍼런스

이 문서는 텍스트를 고차원 벡터로 변환하여 '의미 검색(Semantic Search)'을 가능하게 하는 지식 저장 인프라의 핵심 원리를 다룹니다.

---

## 3.1 기술적 기원: 왜 키워드 매칭을 넘어 의미 기반 검색이 필요한가?
*   **키워드 검색의 한계**: 기존의 형태소 분석 기반 검색은 정확한 단어가 일치해야만 결과를 찾을 수 있어, 유의어나 문맥적 질문에 취합하는 정보를 제공하기 어렵습니다.
*   **비정형 데이터의 차원적 이해**: 수천 수만 장의 문서 중 '내용상 가장 가까운' 것을 찾기 위해 데이터를 숫자로 치환하여 공간상의 좌표(Vector)로 관리할 필요가 생겼습니다.

## 3.2 아키텍처 설계: 지식 수치화의 핵심 정의
*   **Embedding (임베딩)**: 단어와 문장의 의미를 수백 개의 실수 리스트(고차원 벡터)로 압축 변환하는 과정입니다.
*   **Vector Database (벡터 데이터베이스)**: 고차원 벡터들 사이의 거리(유사도)를 전문적으로 계산하고 저장하는 특수 DB입니다.
*   **Cosine Similarity (코사인 유사도)**: 두 벡터 사이의 각도를 이용해 '의미적 거리'를 측정하는 대표적인 수학적 지표입니다.

## 3.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **시맨틱 인덱싱**: 문장의 표면적 단어가 달라도 맥락이 비슷하면 '가까운 좌표'로 정렬하여 검색 성능을 극대화합니다.
*   **고속 기하 연산**: 수백만 개의 문서 조각 중 사용자의 질문과 가장 유사한 상위 K개(Top-K)를 밀리초(ms) 단위로 추출합니다.
*   **영속성 관리**: 휘발될 수 있는 임베딩 데이터를 디스크에 물리적으로 저장하여 시스템 재시작 후에도 지식을 유지합니다.

## 3.4 비즈니스 전략: VectorDB 도입의 실무적 근거
*   **지능적 검색 엔진**: "연차 규정"이라고 물었을 때 "휴가 안내 문서"를 찾아낼 수 있는 유연한 검색 능력을 제공합니다.
*   **LLM의 외부 기억 장치**: 모델의 파라미터 내부에 지식을 우겨넣지 않고, 필요할 때만 외부에서 정확한 정보를 수급하여 할루시네이션을 원천 차단합니다.
*   **데이터 필터링**: 메타데이터를 활용한 필터링(예: 2024년 문서만 검색)을 통해 결과의 정확도를 한 번 더 검증할 수 있습니다.

## 3.5 트레이드오프: 벡터 검색 엔진의 강점과 제약 사항
*   **장점**
    *   **유연성**: 동의어 사전 구축 없이도 의미적 검색이 가능합니다.
    *   **확장성**: 문서의 양이 늘어나도 효율적인 인덱싱 알고리즘을 통해 검색 속도를 유지합니다.
*   **단점**
    *   **모델 의존성**: 사용한 임베딩 모델의 성능이 검색 품질을 100% 결정합니다.
    *   **고유명사 취약**: "딥러닝" 같은 일반 명사는 잘 찾지만, "Project-A12" 같은 고유 명사는 키워드 검색보다 정확도가 낮을 수 있습니다.

## 3.6 비교 우위: 대안 DB 및 검색 방식과의 결정적 차이점
*   **ChromaDB vs Pinecone**:
    *   ChromaDB는 로컬 실행이 가능하여 보안과 실무 테스트에 최적이며, Pinecone은 클라우드 기반으로 대규모 서비스 확장에 유리합니다.
*   **Vector Search vs Keyword Search**:
    *   키워드 검색은 '정확한 단어 매칭'에, 벡터 검색은 '추상적 질문의 의도 이해'에 강점이 있습니다.

## 3.7 연관 생태계: 벡터 검색 및 인덱싱 통합 스택
*   **Chroma**: 파이썬 생태계에서 가장 널리 쓰이는 오픈소스 로컬 벡터 DB.
*   **LangChain VectorStore**: 다양한 벡터 DB를 동일한 인터페이스로 다루게 해주는 래퍼 클래스.
*   **Nomic Embed / OpenAI Embeddings**: 문장을 숫자로 바꿔주는 임베딩 모델군.

---
**실습 연결 포인트**:
- 벡터 DB 구축과 실제 데이터 저장은 `01_basic_rag/tutorial_step3_rag.md`의 `Chroma.from_documents` 코드 부분에서 상세히 다루고 있습니다.


---

# 파트 4. RAG 시스템 구축: 검색 증강 생성의 통합 아키텍처 레퍼런스

이 문서는 검색된 문서 조각과 LLM의 추론 능력을 결합하여 최종 응답을 생성하는 RAG 엔진의 오케스트레이션 원리를 다룹니다.

---

## 4.1 기술적 기원: 왜 생성 모델에 검색을 결합해야 하는가?
*   **LLM의 단일 추론의 한계**: 모델 단독으로는 "사실 관계"보다 "확률적으로 그럴싸한 답변"을 선호하여 거짓 정보를 생산할 위험이 큽니다.
*   **지식의 내재화 vs 외부 수급**: 수조 원이 드는 모델 재학습 대신, 필요한 지식을 즉석에서 '참고 서적'처럼 읽게 하는 것이 훨씬 경제적이고 정확하다는 결론에 도달했습니다.

## 4.2 아키텍처 설계: RAG 엔진 오케스트레이션의 핵심 정의
*   **Prompt Engineering (프롬프트 엔지니어링)**: 검색된 문서와 질문을 LLM이 이해하기 쉬운 구조로 배치하고, 행동 지침(시스템 메시지)을 부여하는 기술입니다.
*   **Retriever (검색기)**: 벡터 DB에서 관련 문서를 찾아오는 소프트웨어 모듈로, 단순 검색을 넘어 검색 결과의 순위를 조정하는 전략이 포함됩니다.
*   **Context Window Management**: LLM이 처리할 수 있는 입력 용량 내에 가장 핵심적인 정보만 선별하여 넣는 최적화 과정입니다.

## 4.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **질문 변환 (Query Transformation)**: 사용자의 모호한 질문을 벡터 검색에 유리한 형태로 다시 작성합니다.
*   **지식 보강 (Augmentation)**: LLM의 기본 지식에 검색된 외부 지식을 결합하여 "주어진 정보만을 바탕으로 답하라"는 강력한 제약을 부여합니다.
*   **최종 응답 생성 (Generation)**: 추론형 모델(DeepSeek 등)이 제공된 근거를 바탕으로 논리적으로 완결된 답변을 작성합니다.

## 4.4 비즈니스 전략: RAG 시스템 도입의 실무적 근거
*   **할루시네이션 방지**: 답변의 모든 문장이 검색된 문서에 근거하도록 설계하여 기업용 서비스의 신뢰도를 확보합니다.
*   **출처 가시성 (Traceability)**: 사용자가 답변 하단에 표시된 원본 문서를 직접 클릭하여 내용을 재확인할 수 있는 투명성을 제공합니다.
*   **도메인 적응**: 일반 모델이 모르는 사내 특유의 용어나 문화를 즉시 서비스에 반영하기 위해 도입되었습니다.

## 4.5 트레이드오프: 통합 RAG 아키텍처의 강점과 제약 사항
*   **장점**
    *   **신뢰성**: 사실 기반의 답변을 보장합니다.
    *   **최신성**: 모델 학습 시점과 상관없이 최신 문서를 참조합니다.
*   **단점**
    *   **지연 시간 (Latency)**: 검색 과정과 LLM 생성 과정이 합쳐져 답변 생성 속도가 순수 모델보다 느려질 수 있습니다.
    *   **검색 품질 의존성**: 검색된 문서가 부족하면 LLM은 "모른다"고 답하거나 성능이 저하됩니다.

## 4.6 비교 우위: 순수 LLM 답변 방식과의 결정적 차이점
*   **순수 LLM vs RAG 엔진**:
    *   순수 LLM은 창의적 글쓰기에 강하지만 사실 관계에 약합니다. RAG 엔진은 창의성보다 '정교한 지식 전달'에 특화되어 있습니다.
*   **Basic RAG vs Advanced Rerank**:
    *   Basic은 단순히 벡터 거리가 가까운 것을 가져오지만, 본 레퍼런스에서 지향하는 방식은 검색된 결과를 다시 한번 정밀 채점하여 최상의 문맥을 선별합니다.

## 4.7 연관 생태계: RAG 엔진 구성 및 스트리밍 통합 스택
*   **Ollama (DeepSeek-R1)**: 고도로 훈련된 로컬 추론 엔진.
*   **LangChain Expression Language (LCEL)**: RAG 파이프라인(`검색 -> 보강 -> 생성`)을 선언적으로 구축하는 문법.
*   **Prompt Template**: 고정된 지시문과 가변적인 문맥/질문을 조합하는 도구.

---
**실습 연결 포인트**:
- 전체 RAG 엔진의 동작 흐름과 프롬프트 구성은 `01_basic_rag/tutorial_step3_rag.md`와 해당 파이썬 코드의 `PromptTemplate` 정의에서 확인할 수 있습니다.


---

# 파트 5. 통합 에이전트 설계: 정형/비정형 데이터 결합 레퍼런스

이 문서는 문서(비정형)와 데이터베이스(정형)를 동시에 다루며, 질문에 따라 적절한 도구를 선택하는 지능형 에이전트 설계 방식을 다룹니다.

---

## 5.1 기술적 기원: 왜 단일 RAG를 넘어 통합 에이전트가 필요한가?
*   **지식의 이원성**: 기업 지식은 '규정(문서)'과 '수치(DB)'라는 두 가지 형태로 공통 존재합니다. "휴가 규정이 뭐야?"는 RAG가 답하지만, "박 대리의 남은 휴가는?"은 DB 조회가 필요합니다.
*   **복합 질문의 증대**: 사용자는 하나의 질문 안에 두 데이터를 모두 요구하는 경우가 많아, 이를 유기적으로 처리할 '두뇌(Agent)'가 필수적입니다.

## 5.2 아키텍처 설계: 지능형 판단 및 도구 연동의 핵심 정의
*   **Agent (에이전트)**: 고정된 흐름(Chain)을 따르지 않고, LLM이 상황에 맞게 어떤 행동을 할지 스스로 추론하고 결정하는 루프를 의미합니다.
*   **Tool Calling / Function Calling**: LLM이 특정 함수(DB 조회, 웹 검색 등)를 실행하기 위해 필요한 인자값을 스스로 생성하는 기술입니다.
*   **Router (라우터)**: 사용자의 의도(Intent)를 분석하여 RAG 검색기로 보낼지, 아니면 DB 조회 도구로 보낼지 결정하는 신호등 역할을 수행합니다. (라우터의 구체적인 구현 방식과 구조는 파트 6. 랭체인에서 상세히 다룹니다.)

## 5.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **의도 파악 (Intent Classification)**: "몇 명인가요?"와 같은 질문에서 '수량 파악(SQL 조회)' 의도를 추출해냅니다.
*   **도구 선택 (Tool Selection)**: 가용한 도구들의 이름과 설명을 읽고, 현재 질문 해결에 가장 적합한 도구를 골라 호출합니다.
*   **정보 통합 (Reasoning & Synthesis)**: 문서에서 얻은 정보와 DB에서 얻은 숫자를 결합하여 하나의 완결된 문장으로 재구성합니다.

## 5.4 비즈니스 전략: 통합 에이전트 도입의 실무적 근거
*   **다목적 해결 능력**: 하나의 채팅창에서 문서 검색부터 이사 결재, 재고 조회까지 모든 업무를 지원하기 위함입니다.
*   **사용자 경험 극대화**: 사용자가 질문의 성격에 따라 메뉴를 고를 필요 없이, 자연어로만 소통해도 시스템이 알아서 판단하게 합니다.
*   **시스템 확장성**: 새로운 도구(예: ERP 연동)가 추가되어도 전체 로직을 바꾸지 않고 에이전트에게 도구 설명만 추가하면 즉시 적용 가능합니다.

## 5.5 트레이드오프: 에이전트 기반 아키텍처의 강점과 제약 사항
*   **장점**
    *   **범용성**: 다양한 데이터 소스를 하나의 앱으로 통합할 수 있습니다.
    *   **지능형 대응**: 예상치 못한 질문에도 가용한 도구를 활용해 유연하게 대처합니다.
*   **단점**
    *   **불확실성**: 루프를 돌면서 예기치 않은 행동을 할 수 있어 제어가 어렵습니다.
    *   **지연 및 비용**: 에이전트가 고민하는 단계(Reasoning)와 여러 번의 도구 호출로 인해 응답 시간이 길어집니다.

## 5.6 비교 우위: 단순 선형 체인(Chain)과의 결정적 차이점
*   **단순 Chain vs Agentic Flow**:
    *   Chain은 정해진 순서(A -> B -> C)만 따르지만, Agent는 질문에 따라 A를 생략하거나 B를 두 번 호출하는 등 유동적으로 움직입니다.
*   **RAG Only vs RAG + MCP**:
    *   RAG 전용 시스템은 텍스트 정보에 갇혀 있지만, 통합 에이전트는 현실 세계의 살아있는 데이터(DB)와 상시 동기화됩니다.

## 5.7 연관 생태계: 에이전트 워크플로우 및 프로토콜 통합 스택
*   **LangGraph**: 상태(State)를 가진 복잡한 에이전트 워크플로우를 설계하기 위한 최상위 도구.
*   **MCP (Model Context Protocol)**: 에이전트가 외부 도구와 소통하는 표준 언어.
*   **PydanticOutputParser**: 에이전트가 결과를 JSON 등 기계가 읽기 쉬운 구조로 내뱉게 강제하는 기술.

---
**실습 연결 포인트**:
- 에이전트의 기본적인 판단 로직과 도구 선택 흐름은 `01_basic_rag/tutorial_step4_reasoning.md`와 해당 파이썬 코드의 에이전트 정의 부분에서 상세히 실습할 수 있습니다.


---

# 파트 6. LangChain 오케스트레이션: 시스템 연결 및 워크플로우 제어 레퍼런스

이 문서는 LLM, VectorDB, API 등을 하나로 묶어 유기적인 서비스를 만드는 통합 프레임워크의 구조와 설계 철학을 다룹니다.

---

## 6.1 기술적 기원: 왜 복잡한 AI 앱에는 전용 오케스트레이션 도구가 필요한가?
*   **컴포넌트의 파편화**: 모델(Ollama), 저장소(Chroma), 웹(FastAPI) 등 각기 다른 기술을 일일이 수동으로 연동하는 것은 개발 생산성을 크게 저해합니다.
*   **기술의 빠른 교체**: 오늘 쓰는 LLM이나 DB가 내일 더 좋은 것으로 바뀌더라도, 비즈니스 로직(코드)은 최소한으로 수정되어야 하는 요구가 생겼습니다.

## 6.2 아키텍처 설계: LangChain의 핵심 구조와 설계 철학
*   **LCEL (LangChain Expression Language)**: 복잡한 연쇄 과정(`프롬프트 -> 모델 -> 파서`)을 파이프(`|`) 기호를 사용하여 직관적이고 선언적으로 작성하는 문법입니다.
*   **Router (라우터)**: 사용자의 질문 의도에 따라 데이터 소스나 처리 경로를 동적으로 분기하는 '신호등' 역할을 수행합니다.
*   **Runnables**: 모든 연결 부품이 공유하는 공통 규격으로, 동기/비동기/스트리밍/배치 연산을 일관되게 처리할 수 있게 합니다.
*   **Memory (메모리)**: 단발성 질문이 아닌 앞선 대화의 흐름을 기억하여 맥락 있는 소통(Multi-turn)을 가능하게 하는 모듈입니다.

## 6.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **지능형 라우팅 (Routing Tactics)**: 
    *   **논리적 라우팅**: LLM이 질문을 분석하여 도구를 선택 (RunnableBranch 활용).
    *   **시맨틱 라우팅**: 질문의 의미 유사도(Embedding)를 계산하여 경로 결정.
    *   **그래프 기반 라우팅**: LangGraph를 이용하여 상태에 따른 복잡한 순환 분기 수행.
*   **표준 인터페이스 제공**: 어떤 LLM 모델을 쓰더라도 같은 `invoke()` 함수로 호출할 수 있는 추상화 계층 역할을 합니다.
*   **데이터 파이프라인 제어**: 입력된 질문을 다듬고(Prompt), 결과물에서 필요한 데이터만 추출(Parser)하는 전체 공정을 자동화합니다.
*   **재시도 및 폴백 (Retry/Fallback)**: 모델 호출이 실패했을 때 자동으로 다시 시도하거나, 대안 모델로 전환하여 시스템 안정성을 유지합니다.

## 6.4 비즈니스 전략: LangChain 도입의 실무적 근거
*   **생태계 연동성**: 수백 개의 VectorDB와 수천 개의 도구가 이미 LangChain용으로 개발되어 있어, 새로운 기술 도입 시 연동 비용이 거의 0에 가깝습니다.
*   **복잡한 워크플로우 관리**: 조건별 분기나 순환 구조를 가진 에이전트 로직을 체계적으로 설계하기 위해 필수적입니다.
*   **코드 유지보수**: 비즈니스 로직과 기술 엔진이 분리되어 있어, 시스템이 커져도 스파게티 코드가 되지 않습니다.

## 6.5 트레이드오프: 오케스트레이션 프레임워크의 강점과 제약 사항
*   **장점**
    *   **강력한 추상화**: 하부 구현체(어떤 DB인지, 어떤 모델인지)를 몰라도 상위 로직을 짤 수 있습니다.
    *   **스트리밍 지원**: 답변이 생성되는 대로 사용자에게 즉시 보여주는 기능을 매우 쉽게 구현합니다.
*   **단점**
    *   **잦은 업데이트**: 라이브러리가 너무 자주 업데이트되어 버전 관리가 까다롭습니다.
    *   **과도한 추상화**: 내부 동작을 깊이 이해하지 않고 쓰면 오류가 났을 때 디버깅이 어려울 수 있습니다.

## 6.6 비교 우위: 대안 프레임워크와의 결정적 차이점
*   **vs 직접 API 호출**: 직접 짜면 가볍지만, 스트리밍, 재시도, 로그 추적 등을 모두 직접 구현해야 하는 막대한 공수가 듭니다.
*   **vs LlamaIndex**: LlamaIndex는 '데이터 검색과 로딩'에 더 깊은 전문성을 가지며, LangChain은 '범용적인 에이전트 워크플로우'와 '애플리케이션 통합'에 더 강점이 있습니다.

## 6.7 연관 생태계: AI 앱 오케스트레이션 통합 스택
*   **LangGraph**: 상태를 가진 순환(Cycle) 그래프 에이전트 설계 도구.
*   **LangSmith**: LLM 앱의 성능을 모니터링하고 디버깅하는 플랫폼.
*   **LCEL**: 체인을 효율적으로 조립하기 위한 전용 언어.

---
**실습 연결 포인트**:
- LCEL을 이용한 체인 구성과 스트리밍 응답 구현은 `01_basic_rag/tutorial_step5_finish.md` 및 해당 단계의 코드에서 핵심적으로 다루고 있습니다.


---

# 파트 7. RAG 튜닝 및 고도화: 답변 품질의 한계를 극복하는 기술 레퍼런스

이 문서는 프로토타입 수준의 RAG를 상용 서비스 수준으로 끌어올리기 위한 검색 정밀도 향상 및 최적화 전략을 다룹니다.

---

## 7.1 기술적 기원: 왜 로컬 RAG는 고도화 튜닝이 필수적인가?
*   **프로토타입의 한계**: 단순히 벡터 검색만으로는 "비슷해 보이지만 틀린" 문서를 가져오거나, 최신성/우선순위 구분을 못 하는 등 현업 적용이 어려운 경우가 빈번합니다.
*   **품질의 임계점**: AI 비서가 "똑똑하다"고 느끼게 하려면 의미 검색의 모호함을 제거하는 정교한 수학적/논리적 필터링 단계가 필요합니다.

## 7.2 아키텍처 설계: 답변 품질 향상을 위한 고급 튜닝 전략
*   **ReRanker (재순위화)**: 벡터 검색으로 빠르게 추출한 상위 N개 문서를, 더 똑똑한 모델(Cross-Encoder)로 다시 한번 정밀 채점하여 순위를 재조정하는 과정입니다.
*   **Hybrid Search**: 벡터 기반의 '의미 검색'과 전통적인 '키워드 매칭(BM25)'의 장점을 가중치(Convex Combination)를 주어 섞어 쓰는 방식입니다.
*   **Query Expansion (질문 확장)**: 사용자의 짧고 모호한 질문을 의미가 풍부한 여러 문장으로 변환하거나, 가상의 답변(HyDE)을 생성하여 검색의 접점을 넓히는 기술입니다.

## 7.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **정밀 타격 (Recall to Precision)**: 1차로 많이 가져온 뒤(재현율), 2차에서 정확한 것만 골라내는(정밀도) 2단계 파이버라인(Two-stage Pipeline)을 가동합니다.
*   **최신성 및 우선순위 필터링**: 메타데이터를 활용하여 최신 날짜의 문서에 가중치를 주거나, 특정 보안 등급의 문서를 제외합니다.
*   **문맥 보강 (Parent Document Retrieval)**: 검색은 '작은 조각'으로 하되, LLM에게는 그 조각이 포함된 '전체 단락(부무 문서)'을 전달하여 의미의 끊김을 방지합니다.

## 7.4 비즈니스 전략: RAG 고도화 도입의 실무적 근거
*   **사용자 신뢰 확보**: 10번 중 1번만 틀려도 사용자는 업무에 AI를 쓰지 않습니다. 그 1번의 오답을 잡기 위해 튜닝이 필요합니다.
*   **전문 용어 대응**: 우리 회사만 쓰는 축약어나 프로젝트명은 일반 벡터 검색만으로는 한계가 있으며, 키워드 검색을 섞어야 답변이 정확해집니다.
*   **응답 일관성**: 유사한 질문에 대해 항상 통계적으로 가장 우수한 근거 문서를 참조하도록 고정하기 위함입니다.

## 7.5 트레이드오프: 튜닝 및 최적화 과정의 강점과 제약 사항
*   **장점**
    *   **압도적 정확도**: 단순 RAG 대비 정답률이 30~50% 이상 향상됩니다.
    *   **데이터 이해도 상승**: 전처리 과정에서 데이터의 문제점(중복, 누락)을 파악하고 정정할 기회가 생깁니다.
*   **단점**
    *   **지연 시간 증가**: ReRanking 과정을 거치면 사용자 응답 속도가 1~2초가량 더 소요될 수 있습니다.
    *   **복잡한 튜닝값**: 키워드와 벡터의 가중치 비율(예: 0.3:0.7)을 찾는 실험적 과정이 필요합니다.

## 7.6 비교 우위: Naive RAG와의 결정적 차이점
*   **Naive RAG vs Advanced RAG**:
    *   Naive는 "가져와서 읽어라" 수준이지만, Advanced는 "가져온 것 중 최고를 골라 읽어라"는 전략이 포함됩니다.
*   **Vector Only vs Hybrid**:
    *   Vector Only는 은유나 상징 이해에 좋고, Hybrid는 특정 품번이나 법령 번호 같은 '정확한 단어' 매칭에 강합니다.

## 7.7 연관 생태계: 검색 품질 고도화 통합 스택
*   **BGE-Reranker / Cohere Rerank**: 검색 결과 가공의 표준 모델.
*   **BM25 (Rank-BM25)**: 고전적이지만 가장 강력한 키워드 기반 코퍼스 점계 알고리즘.
*   **HyDE (Hypothetical Document Embeddings)**: 가상의 답변을 먼저 생성해 검색 효율을 높이는 기법.

---
**실습 연결 포인트**:
- 검색 품질 개선을 위한 고급 청킹 기법과 메타데이터 활용법은 `01_basic_rag/docs/청킹심화.md`의 상세 가이드를 통해 학습할 수 있습니다.


---

# 파트 8. 배포와 운영: 안정적인 AI 서비스 공급 전략 레퍼런스

이 문서는 개발된 AI 에이전트를 실제 사용자가 사용할 수 있는 환경으로 이관하고, 지속 가능하게 관리하기 위한 운영(MLOps) 기술을 다룹니다.

---

## 8.1 기술적 기원: 왜 로컬 AI 에이전트 프로젝트에 운영(MLOps) 관점이 필요한가?
*   **개발과 운영의 간극**: 개발자 PC에서는 잘 돌아가던 AI가 실제 서버 환경(OS 차이, 라이브러리 부재)에서 작동하지 않는 문제를 해결해야 합니다.
*   **자원 가동율과 비용**: LLM은 CPU/GPU 자원을 막대하게 소비하므로, 동시 접속자가 늘어날 때 시스템이 다운되지 않도록 효율적인 자원 배분 전략이 필요해졌습니다.

## 8.2 아키텍처 설계: 인프라 패키징 및 배포의 핵심 정의
*   **Dockerization (도커화)**: 애플리케이션, 런타임, 환경 설정을 하나의 이미지로 묶어 어디서나 동일하게 실행되게 하는 기술입니다.
*   **Docker Compose**: API 서버, VectorDB(Chroma), RDB(Postgres) 등 여러 개의 컨테이너를 하나의 네트워크로 묶어 유기적으로 관리하는 오케스트레이션 도구입니다.
*   **Reverse Proxy (Nginx)**: 클라이언트의 요청을 받아 적절한 서버로 분산(Load Balancing)하고, SSL 보안 인증을 담당하는 관문 역할을 합니다.

## 8.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **환경 일관성 보장**: "내 로컬에서는 됐는데 서버에서는 안 된다"는 문제를 원천 차단합니다.
*   **장애 격리**: DB 컨테이너가 죽더라도 API 서버가 자동으로 복구되거나 다른 자원에 영향을 주지 않도록 격리합니다.
*   **지속적 모니터링**: LLM의 응답 속도(Latency), 토큰 소비량, 시스템 자원(MEM/GPU) 사용량을 실시간으로 감시합니다.

## 8.4 비즈니스 전략: 안정적인 배포 환경 구축의 실무적 근거
*   **상용 서비스 수준의 안정성**: 24시간 중단 없는 서비스를 위해 필수적입니다.
*   **배포 자동화**: 코드 한 줄 수정 후 명령 한 줄(`docker-compose up -d`)로 전체 시스템을 최신 상태로 갱신하기 위함입니다.
*   **데이터 주권의 완성**: 로컬 배포 환경을 구축함으로써 데이터가 외부로 나가지 않는 완벽한 폐쇄형 보안 시스템을 완성합니다.

## 8.5 트레이드오프: 컨테이너 기반 운영의 강점과 제약 사항
*   **장점**
    *   **이식성**: 사내 서버뿐만 아니라 필요시 AWS, Azure 등으로 즉시 이전이 가능합니다.
    *   **버전 관리**: 이전 상태로의 롤백(Rollback)이 매우 빠르고 안전합니다.
*   **단점**
    *   **학습 비용**: 컨테이너 네트워크와 설정 파일(YAML)에 대한 지식이 필요합니다.
    *   **가상화 오버헤드**: 미세하게나마 하드웨어 자원을 직접 쓰는 것보다 비효율이 발생할 수 있으나 실무에서는 무시할 만한 수준입니다.

## 8.6 비교 우위: 대안 배포 및 운영 방식과의 결정적 차이점
*   **단순 실행 vs 컨테이너 운영**: 직접 실행은 간편하지만 의존성 충돌과 좀비 프로세스 관리에 취약한 반면, 컨테이너 운영은 체계적인 수명 주기(Lifecycle) 관리를 지원합니다.
*   **On-Prem vs Serverless**:
    *   서버리스(Lambda 등)는 LLM의 긴 실행 시간과 무거운 이미지 크기를 감당하기 어렵지만, 컨테이너 방식은 장시간 대규모 연산에 최적화되어 있습니다.

## 8.7 연관 생태계: 패키징 및 상용 운영 통합 스택
*   **Docker / Docker Compose**: 컨테이너 인프라의 표준.
*   **Nginx**: 고성능 리버스 프록시 및 로드 밸런서.
*   **Prometheus & Grafana**: 시스템 매트릭 수집 및 시각화 도구.

---
**실습 연결 포인트**:
- 전체 시스템을 하나로 묶어 배포하는 Docker 환경 구성은 프로젝트 루트의 `docker-compose.yml` 설정과 개별 파트의 배포 체크리스트를 통해 완성할 수 있습니다.


---

# Q&A: 핵심 질문 리스트

이 문서는 RAG/LLM 시스템 구축 과정에서 자주 발생하는 질문과 기술 면접/학습 점검용 답변을 정리했습니다.

---

## PART 1~3: 인프라 및 데이터

### Q1. VectorDB는 RDB(SQL)와 무엇이 다른가요?
- **RDB**는 정확한 값(키워드, ID)을 매칭하는 데 특화되어 있고, **VectorDB**는 데이터의 **의미적 유사도(거리)**를 계산하는 데 특화되어 있습니다.
- 예를 들어, RDB에서 `SELECT * FROM docs WHERE content LIKE '%휴가%'`를 하면 "연차"라는 단어는 못 찾지만, VectorDB는 "휴가"와 "연차"의 벡터 거리가 가까우므로 찾아낼 수 있습니다.

### Q2. 임베딩(Embedding)이란 무엇이며 왜 필요한가요?
- 컴퓨터는 사람의 언어(한글/영어)를 이해하지 못합니다. 임베딩은 텍스트를 **실수형 벡터(숫자 리스트)**로 변환하여 컴퓨터가 계산할 수 있게 만드는 과정입니다.
- 의미가 비슷한 단어는 벡터 공간에서 가까운 위치에 좌표가 찍히게 됩니다.

### Q3. 청킹(Chunking) 사이즈는 어떻게 정해야 하나요?
- 정답은 없지만, 일반적으로 **500~1000자(Token 기준 256~512)** 정도가 적당합니다.
- 너무 작으면 문맥(Context)이 잘려서 이해하기 어렵고, 너무 크면 LLM의 Context Window를 많이 차지하고 검색 정밀도가 떨어집니다. 보통 **Overlap(중복 구간)**을 10~20% 두어 문맥 끊김을 방지합니다.

---

## PART 4~5: RAG 및 에이전트

### Q4. RAG를 썼는데도 할루시네이션(거짓말)을 합니다. 왜 그런가요?
- **검색 실패**: 애초에 이상한 문서를 가져왔을 경우 (Garbage In).
- **LLM의 고집**: LLM이 검색된 문서보다 자기가 학습한 지식(Pre-trained knowledge)을 더 우선시할 경우.
- **해결책**: 프롬프트에 "반드시 주어진 문서에서만 답하라", "문서에 없으면 모른다고 하라"고 강력하게 지시하거나, **ReRanker**를 도입해 검색 품질을 높여야 합니다.

### Q5. Router/Agent는 언제 필요한가요?
- 사용자의 질문이 **복합적**일 때 필요합니다.
- 예: "지난달 A팀 매출(DB 조회)이랑 우리 회사 휴가 규정(문서 검색) 알려줘."
- 단순 RAG는 DB를 볼 수 없고, 단순 SQL 봇은 문서를 볼 수 없으므로, 질문 의도를 판단해 적절한 도구를 연결해주는 **Router**가 필수적입니다.

---

## PART 6~7: 튜닝 및 최적화

### Q6. LangChain이 너무 복잡합니다. 꼭 써야 하나요?
- 필수는 아닙니다. 단순한 API 호출은 `requests`나 `openai` 패키지로 직접 짜는 게 더 빠를 수 있습니다.
- 하지만 문서 로더(PDF, Docx 등), 텍스트 분할기, VectorDB 연동, 메모리 관리 등 **공통 기능을 직접 구현하는 비용**이 크기 때문에, 복잡한 앱을 만들 때는 LangChain 같은 프레임워크가 장기적으로 유리합니다.

### Q7. ReRanker를 쓰면 속도가 느려지지 않나요?
- 네, 느려집니다. 1차 검색(Vector)은 0.01초 단위지만, ReRanker(Cross-Encoder)는 문서 하나하나를 모델에 통과시키므로 수십 배 느립니다.
- 따라서 **1차에서 50~100개를 빠르게 추리고(Recall)**, **2차에서 상위 5~10개만 정밀하게 재순위화(Precision)**하는 **2-Pass 전략**을 사용합니다.

### Q8. Hybrid Search(하이브리드 검색)는 무엇인가요?
- **Vector Search(의미)** + **Keyword Search(단어 매칭)**을 결합한 것입니다.
- "2024년" 같은 특정 연도나 "갤럭시 S24" 같은 특정 모델명은 키워드 검색이 정확하고, "배터리 오래 가는 폰 추천해줘" 같은 질문은 벡터 검색이 유리합니다. 이 둘의 장점을 합치는 것입니다.

---

## PART 8: 운영

### Q9. 로컬 LLM을 서비스에 쓸 때 가장 큰 문제는 무엇인가요?
- **동시성(Concurrency)**입니다. ChatGPT API는 수천 명이 동시에 써도 되지만, 로컬 GPU/CPU는 한 번에 하나의 요청을 처리하는 데도 자원을 많이 씁니다.
- 사용자가 몰리면 대기 시간이 길어지므로, **vLLM** 같은 고성능 서빙 프레임워크를 쓰거나 큐(Queue) 시스템을 잘 설계해야 합니다.

### Q10. 데이터 보안 측면에서 로컬 RAG의 장점은?
- 사내의 민감한 데이터(연봉, 계약서 등)가 OpenAI나 Google 서버로 전송되지 않고, **물리적으로 차단된 사내 서버(On-Premise)** 안에서만 맴돌기 때문에 보안 규정이 엄격한 금융/공공/기업 환경에 적합합니다.
