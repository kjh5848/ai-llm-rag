# PART 0. 이 책의 목표와 핵심 아키텍처

## 1. 📌 탄생 배경 (Background)
- **문제점**: 기존 LLM(ChatGPT 등)은 범용적인 지식은 뛰어나지만, **"폐쇄된 사내 데이터"**를 알지 못함.
- **왜 지금 Fine-tuning이 아닌가?**:
  - 비용 문제보다는 **적합한 단계의 문제**임. 데이터가 수시로 바뀌는 환경(규정 변경, 신규 직원)에서 매번 재학습(Fine-tuning)을 하는 것은 비효율적.
  - 지금은 모델에게 "암기"를 시키기보다 "실시간으로 책을 찾아보는 법"을 가르치는 **RAG**가 필수적인 단계.
- **그럼 언제 Fine-tuning을 해야 하는가? (Next Level)**:
  - **도메인 특화**: 사내 약어, 은어, 전문 용어가 너무 많아 일반 모델이 문맥을 이해하지 못할 때.
  - **스타일 일치 (Tone & Manner)**: 회사의 보고서 양식이나 특유의 응답 스타일을 완벽하게 따라야 할 때.
  - **경량화**: 거대한 모델의 추론 능력을 작은 모델(SLM)에 이식하여 비용을 줄이고 싶을 때.
- **해법**: RAG와 MCP로 **"지식을 연결"**하는 시스템을 먼저 구축한 뒤, 필요에 따라 Fine-tuning으로 **"지능을 특화"**시키는 것이 올바른 순서.

## 2. 🔑 개념 (Concept)
- **RAG (Retrieval-Augmented Generation)**: LLM이 답변을 생성하기 전에, 외부 데이터베이스(VectorDB)에서 관련 정보를 **검색(Retrieval)**하여 참고 자료로 제공하는 기술.
- **MCP (Model Context Protocol)**: LLM이 데이터베이스(SQL)나 API 같은 외부 도구를 표준화된 방식으로 연결하여 사용할 수 있게 하는 **연결 프로토콜**.

## 3. ⚙️ 역할 (Role)
- **사내 지식 베이스**: 흩어진 문서(PDF, Word, HWP)를 검색 가능한 지식으로 변환.
- **업무 자동화**: 정형 데이터(매출, 연차) 조회와 비정형 데이터(규정) 검색을 통합하여 직원의 반복 질문 해결.

## 4. 💡 도입 이유 (Reason)
- **데이터 보안**: 로컬 LLM(Ollama)을 사용하여 데이터가 외부(OpenAI 등)로 유출되지 않음.
- **최신성 유지**: 모델 재학습 없이 문서만 업데이트하면 즉시 AI가 새로운 내용을 알게 됨.
- **비용 효율성**: 값비싼 GPU 클러스터 없이 16GB RAM 수준의 일반 워크스테이션에서 구축 가능.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **할루시네이션 감소**: 근거 문서를 보고 답하므로 거짓말이 줄어듦.<br>- **보안**: 사내망 내부에서 완결적인 시스템 구축 가능.<br>- **확장성**: 문서뿐만 아니라 DB, API 등 다양한 소스 연결 가능. |
| **단점** | - **초기 구축 복잡성**: VectorDB, 임베딩, 검색 파이프라인 구축 필요.<br>- **검색 품질 의존도**: 검색이 실패하면 답변도 실패함 (Garbage In, Garbage Out). |

## 6. 🆚 차별점 (Differences)
- **vs Fine-tuning**: 
  - Fine-tuning은 "시험 공부"를 다시 시키는 것(지식 내재화).
  - RAG는 "오픈북 테스트"를 보게 하는 것(참고 자료 활용). 사내 업무에는 RAG가 훨씬 효율적.
- **vs ChatGPT Enterprise**:
  - ChatGPT는 데이터가 외부로 전송될 수 있다는 우려가 존재하지만, 이 아키텍처는 **100% 로컬(On-Premise)** 구동이 가능.

## 7. 🔗 관련 기술 (Related Tech)
- **Ollama / DeepSeek-R1**: 로컬 구동 가능한 고성능 오픈소스 LLM.
- **LangChain**: LLM과 검색기, 도구를 연결하는 오케스트레이션 프레임워크.
- **ChromaDB**: 문서 임베딩을 저장하는 경량 벡터 데이터베이스.

---

# PART 0.5. 개발 환경 설정

## 1. 📌 탄생 배경 (Background)
- **로컬 LLM의 부상**: 클라우드 의존 없이 내 PC에서 LLM을 돌릴 수 있는 기술(Ollama, Llama.cpp)이 발전함에 따라, 개인/사내 서버에서도 AI 서비스 구축이 가능해짐.
- **하드웨어 제약**: 고가의 H100 GPU 없이도 **RAM 16GB** 수준의 일반 장비에서 돌아가는 효율적인 아키텍처가 필요.

## 2. 🔑 개념 (Concept)
- **로컬 호스팅 (Local Hosting)**: 외부 API(OpenAI 등)를 호출하지 않고, 내 컴퓨터의 연산 자원(CPU/GPU/RAM)을 사용하여 모델을 직접 실행하는 방식.
- **가상환경 (Virtual Environment)**: Python 프로젝트 간 라이브러리 충돌을 방지하기 위한 격리된 실행 환경.

## 3. ⚙️ 역할 (Role)
- **인프라 기반**: RAG 시스템이 구동될 물리적/소프트웨어적 토대 마련.
- **모델 서빙**: DeepSeek-R1(텍스트), LLaVA(비전) 등 AI 모델을 메모리에 로드하고 API로 제공.

## 4. 💡 도입 이유 (Reason)
- **실습의 재현성**: 모든 독자가 동일한 환경(Python 버전, 라이브러리)에서 실패 없이 코드를 실행하기 위함.
- **비용 통제**: API 요금 걱정 없이 무제한으로 AI를 테스트하기 위함.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **데이터 주권**: 내 데이터가 내 PC를 떠나지 않음.<br>- **유지보수**: 인터넷이 끊겨도 서비스 가능. |
| **단점** | - **하드웨어 리소스**: RAM 16GB 이상 필요 (DeepSeek + LLaVA 동시 구동 시 부하).<br>- **초기 설정**: Docker, Python, Ollama 등 설치 과정이 번거로움. |

## 6. 🆚 차별점 (Differences)
- **vs 클라우드 환경 (Colab/AWS)**:
  - 클라우드는 설정이 편하지만 비용이 계속 발생하고 데이터 보안 설정이 복잡함.
  - 로컬 환경은 초기 설정만 넘기면 영구적으로 무료에 가까움.

## 7. 🔗 관련 기술 (Related Tech)
- **Ollama**: 가장 쉬운 로컬 LLM 실행 도구. 모델 다운로드/실행을 CLI 한 줄로 처리.
- **Docker**: DB(PostgreSQL), VectorDB(Chroma) 등을 컨테이너로 띄워 환경 종속성 제거.
- **PostgreSQL**: 정형 데이터를 저장할 관계형 데이터베이스.

---

# PART 1. FastAPI로 "초간단 사내 시스템" 만들기

## 1. 📌 탄생 배경 (Background)
- **AI와 레거시의 연결**: AI(두뇌)만 있어서는 아무것도 할 수 없음. 실제 회사의 데이터(직원, 매출, 휴가)를 담고 있는 **"기존 시스템"**이 있어야 AI가 유의미한 비서 역할을 할 수 있음.
- **Python의 부상**: AI 라이브러리(LangChain, PyTorch)가 대부분 Python 기반이므로, 백엔드 서버도 Python(FastAPI)으로 통일하는 것이 생산성에 유리.

## 2. 🔑 개념 (Concept)
- **FastAPI**: Python 기반의 현대적이고 빠른 웹 프레임워크. 비동기 처리(Asyncio)를 지원하여 AI 요청 처리에 적합.
- **CRUD (Create, Read, Update, Delete)**: 데이터 관리의 기본이 되는 4가지 기능. 사내 시스템의 근간.

## 3. ⚙️ 역할 (Role)
- **데이터 공급처**: AI가 조회할 정형 데이터(직원 정보, 휴가 DB)를 생성하고 관리.
- **테스트베드**: "김대리 연차가 몇 개야?"라고 물었을 때, AI가 실제로 조회해올 수 있는 **살아있는 DB** 제공.

## 4. 💡 도입 이유 (Reason)
- **가벼움과 속도**: Django보다 가볍고, Flask보다 빠르며, 자동 문서화(Swagger UI)가 강력함.
- **RAG/MCP 연동 용이성**: Python 함수를 바로 Tool로 변환하기 쉬움.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **생산성**: 코드 몇 줄로 API 서버가 완성됨.<br>- **확장성**: 추후 LangChain 서버와 통합하기 쉬움.<br>- **문서화**: `/docs` 엔드포인트에서 API 테스트 가능. |
| **단점** | - **ORM 복잡성**: SQLAlchemy 등의 DB 도구를 추가로 익혀야 함.<br>- **대규모 트래픽**: Go나 Java에 비해 처리량이 낮을 수 있으나, 사내 툴로는 충분함. |

## 6. 🆚 차별점 (Differences)
- **vs Django**: Django는 너무 무겁고 기능이 많아(Full-stack), 마이크로서비스나 AI API 서버로는 FastAPI가 더 적합.
- **vs Flask**: Flask는 동기식 처리가 기본이라 AI 스트리밍이나 비동기 도구 호출에 불리함.

## 7. 🔗 관련 기술 (Related Tech)
- **Jinja2**: 서버 사이드 템플릿 엔진. React 같은 프론트엔드 없이 빠르게 HTML 화면을 만들 때 사용.
- **Uvicorn**: 비동기 Python 웹 서버(ASGI).
- **SQLAlchemy**: Python 코드와 SQL DB를 연결해주는 ORM 라이브러리.

---

# PART 2. 사내 문서 수집 전략과 문서 표준 만들기

## 1. 📌 탄생 배경 (Background)
- **Garbage In, Garbage Out**: 아무리 좋은 AI 모델을 써도, **"입력되는 문서가 엉망이면 AI의 답변도 엉망"**임.
- **비정형 데이터의 난립**: 회사에는 포맷도 제각각(HWP, PDF, Excel), 버전도 제각각(v1_최종_진짜최종.docx)인 문서가 흩어져 있음.

## 2. 🔑 개념 (Concept)
- **데이터 파이프라인 (Data Pipeline)**: 문서를 수집 -> 정제 -> 변환하는 일련의 흐름.
- **문서 표준화 (Standardization)**: 제목, 섹션, 버전 등을 통일하여 기계가 읽기 쉽게 만드는 작업.

## 3. ⚙️ 역할 (Role)
- **지식의 원천**: RAG가 검색할 수 있는 '고품질 연료'를 준비.
- **메타데이터 부여**: 누가, 언제, 왜 만든 문서인지 꼬리표를 달아 검색 정확도 향상.

## 4. 💡 도입 이유 (Reason)
- **검색 품질 향상**: "제목"과 "섹션"이 명확해야 청킹(Chunking)이 잘 되고, 검색도 잘 됨.
- **버전 관리**: 최신 규정(2024년)과 옛날 규정(2020년)이 섞여 있을 때, 최신 문서를 우선시하기 위함.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **AI 성능 획기적 개선**: 모델 튜닝보다 문서 정리가 성능 향상에 더 큰 영향을 줌.<br>- **유지보수 용이**: 폴더 구조가 잡히면 나중에 문서를 추가/삭제하기 편함. |
| **단점** | - **인력 투입**: 기존 문서를 정리하는 데 시간과 노력이 많이 듬(Data Cleaning Cost).<br>- **변환 이슈**: HWP나 표가 많은 PDF는 텍스트 추출이 기술적으로 어려움. |

## 6. 🆚 차별점 (Differences)
- **vs 단순 파일 저장**:
  - 단순 저장은 "파일 덩어리"일 뿐이지만, 표준화된 문서는 "구조화된 데이터"로 취급됨.
- **vs 키워드 검색**:
  - 기존 검색은 파일명만 보지만, RAG를 위한 문서는 내용의 구조(헤더, 문단)까지 고려해야 함.

## 7. 🔗 관련 기술 (Related Tech)
- **Markdown (.md)**: 기계가 읽기 가장 좋은 텍스트 포맷. RAG 구축 시 선호되는 타겟 포맷.
- **python-docx / pypdf**: Word와 PDF에서 텍스트를 추출하는 파싱 라이브러리.
- **Metadata Tagging**: 파일명이나 헤더 정보를 별도 필드로 저장하는 기법.

---

# PART 3. VectorDB 구축: 문서를 “검색 가능한 지식”으로 바꾸기

## 1. 📌 탄생 배경 (Background)
- **컴퓨터의 언어**: 컴퓨터는 한글이나 영어를 직접 이해하지 못함. **"숫자(Vector)"**로 바꿔줘야 계산(비교)이 가능함.
- **의미 검색의 필요성**: "휴가"를 검색했을 때 "연차", "리프레시" 같은 **유의어**까지 찾으려면 단어 매칭이 아닌 **의미적 거리**를 계산해야 함.

## 2. 🔑 개념 (Concept)
- **임베딩 (Embedding)**: 텍스트를 고차원의 벡터(숫자 좌표)로 변환하는 과정. (예: 사과=[0.1, 0.2], 배=[0.11, 0.21])
- **청킹 (Chunking)**: 긴 문서를 AI가 읽기 좋은 크기(문단 단위 등)로 자르는 기술.
- **VectorDB**: 이러한 벡터들을 저장하고, 질문 벡터와 가장 가까운 문서를 빠르게 찾아주는 전용 데이터베이스.

## 3. ⚙️ 역할 (Role)
- **장기 기억장치 (Long-term Memory)**: AI에게 필요한 지식을 영구적으로 저장.
- **고속 검색기**: 수만 개의 문서 중에서 질문과 관련된 Top-K개를 0.1초 안에 찾아냄.

## 4. 💡 도입 이유 (Reason)
- **Context Window 한계 극복**: LLM 입력창에 문서를 다 넣을 수 없으니, **"필요한 것만 쏙 뽑아서"** 넣기 위해 필수적임.
- **비용 절감**: 전체 문서를 매번 모델에 넣으면 토큰 비용이 폭발함.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **시맨틱 검색**: 단어가 달라도 의미가 통하면 찾아냄.<br>- **확장성**: 문서가 늘어나도 검색 속도가 거의 느려지지 않음. |
| **단점** | - **업데이트 비용**: 문서가 수정되면 다시 임베딩하여 DB를 갱신해야 함.<br>- **디버깅 어려움**: 왜 이 문서가 검색되었는지 사람이 직관적으로 이해하기 힘들 때가 있음(블랙박스). |

## 6. 🆚 차별점 (Differences)
- **vs RDB (SQL)**:
  - RDB는 "정확한 값(WHERE id=1)"을 찾지만, VectorDB는 "가장 비슷한 값(Nearest Neighbor)"을 찾음.
- **vs Elasticsearch (Keyword Search)**:
  - 엘라스틱은 "단어 빈도(TF-IDF)" 기반이지만, VectorDB는 "문맥 의미(Embedding)" 기반임.

## 7. 🔗 관련 기술 (Related Tech)
- **Chroma**: 오픈소스이며 파일 기반으로 가볍게 쓸 수 있는 로컬 VectorDB.
- **RecursiveCharacterTextSplitter**: 문맥을 해치지 않으면서 글자 수 기준으로 문서를 자르는 도구.
- **nomic-embed-text / ko-sroberta**: 한국어 처리에 강한 임베딩 모델.

---

# PART 4. RAG로 Q&A 엔진 만들기

## 1. 📌 탄생 배경 (Background)
- **지식의 연결**: VectorDB에 저장된 지식과 LLM의 언어 능력을 실제로 연결하는 **"실행 단계"**.
- **할루시네이션 제어**: 단순히 모델에게 물어보면 거짓말을 하므로, **"검색된 문서만 보고 답해"**라고 강제할 필요가 있음.

## 2. 🔑 개념 (Concept)
- **Retriever (검색기)**: VectorDB에서 사용자의 질문과 관련된 문서를 가져오는 모듈.
- **Augmentation (증강)**: 검색된 문서를 프롬프트(Prompt)에 끼워 넣는 행위.
- **Generation (생성)**: 증강된 프롬프트를 보고 LLM이 최종 답변을 만드는 과정.

## 3. ⚙️ 역할 (Role)
- **질의응답 시스템의 두뇌**: 사용자의 질문을 이해하고, 근거를 찾고, 말을 만드는 전체 프로세스를 담당.
- **출처 추적**: 답변이 어느 문서의 몇 페이지에서 나왔는지 사용자에게 알려주는 역할.

## 4. 💡 도입 이유 (Reason)
- **신뢰성 확보**: 사용자는 AI의 답변만 보고는 믿을 수 없음. **"출처(Source)"**가 함께 제시되어야 업무에 쓸 수 있음.
- **답변 스타일 조정**: 프롬프트 엔지니어링을 통해 "친절하게", "간결하게", "JSON 포맷으로" 등 답변 형식을 제어 가능.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **정확도**: 근거 기반 답변으로 신뢰도가 높음.<br>- **유연성**: 프롬프트만 바꾸면 챗봇의 페르소나(Persona)를 쉽게 변경 가능. |
| **단점** | - **프롬프트 민감성**: 프롬프트를 조금만 잘못 짜도 답변 품질이 크게 달라짐.<br>- **Context Length**: 검색된 문서가 너무 많으면 프롬프트 길이를 초과하여 에러 발생 가능. |

## 6. 🆚 차별점 (Differences)
- **vs 검색 엔진 (Google)**: 
  - 검색 엔진은 "문서 링크"를 주지만, RAG 엔진은 "읽고 요약된 답변"을 줌.
- **vs 챗봇 (Rule-based)**:
  - 기존 챗봇은 정해진 시나리오만 답하지만, RAG는 데이터에 있는 모든 내용에 대해 유연하게 답함.

## 7. 🔗 관련 기술 (Related Tech)
- **RetrieveQA Chain**: LangChain에서 제공하는 RAG 표준 파이프라인.
- **Prompt Template**: {context}와 {question}을 받아 LLM에게 보낼 최종 명령어를 만드는 틀.
- **Streaming Response**: 답변을 한 글자씩 실시간으로 보여주어 체감 속도를 높이는 기술.

---

# PART 5. 정형 MCP, 비정형 VectorDB+RAG: 통합 에이전트 설계

## 1. 📌 탄생 배경 (Background)
- **RAG의 한계**: RAG는 텍스트(비정형) 검색은 잘하지만, "이번 달 총 매출은?" 같은 **정확한 숫자 계산/집계**는 못함 (할루시네이션 발생).
- **정형 데이터 활용**: 회사 정보의 절반은 DB(SQL)에 있음. 이를 AI가 활용하려면 RAG가 아닌 다른 방식(Tool/SQL)이 필요.

## 2. 🔑 개념 (Concept)
- **Agent (에이전트)**: 스스로 도구(Tool)를 선택하고 판단하여 행동하는 AI.
- **Routing (라우팅)**: 질문의 의도를 파악하여 적절한 처리 경로(DB 조회 vs 문서 검색)로 보내는 기술.
- **MCP (Tool)**: AI가 외부 함수(DB Query, API Call)를 실행할 수 있게 해주는 인터페이스.

## 3. ⚙️ 역할 (Role)
- **기술적 3B (Brain, Body, Bridge)**: LLM(Brain)이 도구(Body)를 사용하여 외부 시스템(Bridge)과 상호작용.
- **통합 해결사**: "김대리 연차(DB) 몇 개 남았고, 규정(문서)은 뭐야?" 같은 복합 질문 해결.

## 4. 💡 도입 이유 (Reason)
- **정확성 분리**: 
  - 숫자는 **DB**에서 정확하게 가져와야 함 (사실).
  - 설명은 **문서**에서 찾아야 함 (맥락).
  - 이 둘을 섞어서 완벽한 답변을 주기 위함.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **전지전능함**: 정형/비정형 데이터를 가리지 않고 답변 가능.<br>- **확장성**: 날씨 API, 주식 API 등 도구만 추가하면 능력이 무한 확장됨. |
| **단점** | - **복잡도**: 라우팅 로직이 꼬이면 엉뚱한 도구를 호출할 수 있음.<br>- **속도**: 판단 단계가 추가되어 단순 RAG보다 응답 속도가 느림. |

## 6. 🆚 차별점 (Differences)
- **vs Simple RAG**: 
  - RAG는 "읽기"만 가능하지만, Agent는 "행동(조회, 계산, 실행)"이 가능함.
- **vs SQL Chatbot**: 
  - SQL 챗봇은 문서 내용을 모르지만, 통합 에이전트는 문서와 DB를 모두 앎.

## 7. 🔗 관련 기술 (Related Tech)
- **RouterChain**: 질문 분류기 (예: "매출" 키워드 -> SQL 도구, "규정" 키워드 -> RAG 도구).
- **Function Calling**: LLM이 자연어를 보고 실행해야 할 함수명과 인자를 JSON으로 뽑아주는 기능.
- **SQLAgent**: 자연어를 SQL 쿼리로 변환하여 DB를 조회하는 LangChain 에이전트.

---

# PART 6. LangChain으로 연결 전략 세팅

## 1. 📌 탄생 배경 (Background)
- **점조직의 한계**: RAG 엔진(검색), MCP 도구(DB 조회), LLM(생성)이 각각 따로 놀면 유기적인 서비스가 불가능함.
- **오케스트레이션 필요성**: 이들을 한 줄로 꿰어서 "질문 -> 판단 -> 검색/조회 -> 답변"이라는 워크플로우를 관리할 **지휘자**가 필요함.

## 2. 🔑 개념 (Concept)
- **LangChain**: LLM을 이용한 애플리케이션 개발을 돕는 프레임워크. 다양한 모듈(Model, Prompt, Index, Chain)을 레고 블록처럼 조립 가능.
- **Router (라우터)**: 사용자의 질문을 분석하여 "이건 검색이 필요해", "이건 DB 조회가 필요해"라고 경로를 지정해주는 신호등.
- **Runnable**: LangChain의 모든 컴포넌트가 따르는 표준 인터페이스(invoke, stream, batch).

## 3. ⚙️ 역할 (Role)
- **접착제 (Glue)**: 서로 다른 라이브러리(Chroma, Ollama, SQLAlchemy)를 하나의 인터페이스로 통합.
- **워크플로우 관리자**: 에러 발생 시 재시도(Retry), 스트리밍 응답, 메모리(대화 기록) 관리.

## 4. 💡 도입 이유 (Reason)
- **표준화**: LLM 모델을 GPT-4에서 DeepSeek-R1으로 바꾸더라도 코드 수정(Chain 구조)을 최소화할 수 있음.
- **생산성**: 복잡한 비동기 처리나 프롬프트 관리를 직접 구현하려면 시간이 많이 걸림.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **생태계**: 거의 모든 LLM, VectorDB, Tool과 연동되는 방대한 통합 라이브러리 보유.<br>- **추상화**: 복잡한 로직을 Chain 하나로 묶어서 관리 가능. |
| **단점** | - **러닝커브**: 개념(LCEL 등)이 방대하고 자주 바뀌어서 배우기 어려움.<br>- **오버헤드**: 단순한 API 호출에 비해 코드가 무거워질 수 있음. |

## 6. 🆚 차별점 (Differences)
- **vs Raw Python Code**:
  - 직접 짜면 가볍지만, 기능을 추가할 때마다 스파게티 코드가 되기 쉬움. LangChain은 구조화된 확장이 가능.
- **vs LlamaIndex**:
  - LlamaIndex는 **"데이터 인덱싱/검색"**에 특화되어 있고, LangChain은 **"범용적인 앱 로직/Agent"** 구성에 강점이 있음. (둘을 섞어 쓰기도 함)

## 7. 🔗 관련 기술 (Related Tech)
- **LCEL (LangChain Expression Language)**: `prompt | llm | parser` 처럼 파이프라인을 선언적으로 작성하는 문법.
- **LangGraph**: 순환(Cycle)이 있는 복잡한 에이전트 흐름을 제어하기 위한 LangChain의 상위 도구.

---

# PART 7. RAG 튜닝: "되는 수준"에서 "쓸만한 수준"으로

## 1. 📌 탄생 배경 (Background)
- **초기 환멸 단계**: RAG 프로토타입을 만들면 처음엔 신기하지만, 조금만 복잡한 질문을 던지면 엉뚱한 문서를 가져오거나(거짓 양성), 정작 필요한 문서를 못 찾음(거짓 음성).
- **품질의 벽**: "그냥 검색"이 아니라 **"정확한 검색"**을 위해 검색 파이프라인의 각 단계를 최적화해야 함.

## 2. 🔑 개념 (Concept)
- **ReRanker (재순위화)**: 1차로 빠르게 검색한 50개 문서(Vector Search)를, 느리지만 똑똑한 모델(Cross-Encoder)로 정밀하게 다시 채점하여 순서를 바꾸는 기술.
- **Hybrid Search (하이브리드 검색)**: 의미 기반(Vector) 검색과 키워드 기반(BM25) 검색을 섞어서 사용하는/앙상블하는 기법.
- **Parent Document Retriever**: 검색은 '작은 조각'으로 하되, LLM에게는 그 조각이 포함된 '큰 문맥(부모 문서)'을 던져주는 전략.

## 3. ⚙️ 역할 (Role)
- **정밀 타격**: 사용자의 모호한 질문 속에서도 "진짜 의도"에 맞는 문서를 찾아냄.
- **문맥 보강**: 잘린 문장(Chunk)만으로는 부족한 앞뒤 맥락을 보완하여 LLM에게 전달.

## 4. 💡 도입 이유 (Reason)
- **Vector의 한계**: "2024년 HR 규정"이라고 검색했을 때, Vector는 "2020년 HR 규정"도 내용이 비슷하므로 가져옴(최신성 무시). 이를 필터링해야 함.
- **고유명사 인식**: 사람 이름이나 제품명은 의미(Vector)보다 정확한 철자(Keyword) 매칭이 중요함.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **사용자 경험(UX) 급상승**: "AI가 내 말을 찰떡같이 알아듣네"라는 느낌을 줌.<br>- **신뢰도**: 답변의 근거가 정확해지므로 할루시네이션이 자연스럽게 줄어듦. |
| **단점** | - **속도 저하**: ReRanking이나 Hybrid Search는 연산량이 많아 응답 속도가 느려짐.<br>- **복잡성**: 파이프라인이 길어지고 튜닝할 파라미터(Weight, k값)가 많아짐. |

## 6. 🆚 차별점 (Differences)
- **vs Basic RAG**:
  - Basic: 질문 -> VectorDB -> LLM (단순)
  - Advanced: 질문 -> Query변환 -> Hybrid검색 -> ReRanking -> LLM (정교)

## 7. 🔗 관련 기술 (Related Tech)
- **BGE-Reranker**: 오픈소스로 사용 가능한 고성능 ReRanking 모델.
- **BM25**: 전통적인 검색 엔진(Elasticsearch, Solr)에서 쓰는 키워드 스코어링 알고리즘.
- **Self-Query Retriever**: LLM이 질문을 분석해 메타데이터 필터(Filter)를 자동으로 생성하는 기술.

---

# PART 8. 배포와 운영

## 1. 📌 탄생 배경 (Background)
- **로컬호스트의 탈출**: `localhost:8000`에서만 돌아가는 서비스는 나 혼자밖에 못 씀. 팀원들이 쓰려면 서버에 배포해야 함.
- **안정성 이슈**: 개발 중엔 에러 나면 끄고 다시 켜면 되지만, 운영 환경에선 **"죽지 않는 서비스"**가 되어야 함.

## 2. 🔑 개념 (Concept)
- **Docker**: 내 컴퓨터 환경(OS, 라이브러리)을 그대로 컨테이너에 담아 어느 서버에서든 똑같이 실행되게 하는 기술.
- **Docker Compose**: 여러 컨테이너(API서버, DB, VectorDB)를 한 번에 실행하고 관리하는 도구.
- **Nginx**: 사용자의 요청을 받아서 백엔드 서버로 전달해주는 문지기(Reverse Proxy). 보안(HTTPS)과 부하 분산을 담당.

## 3. ⚙️ 역할 (Role)
- **서비스화**: 24시간 365일 중단 없이 서비스를 제공하기 위한 환경 구축.
- **모니터링**: 누가 언제 썼는지, 에러는 없는지, LLM이 이상한 답을 하지는 않는지 감시.

## 4. 💡 도입 이유 (Reason)
- **환경 일치**: 개발에선 됐는데 운영에선 안 되는("It works on my machine") 문제를 해결.
- **확장성**: 사용자가 늘어나면 컨테이너 개수만 늘려서 대응 가능.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **이식성**: AWS, Azure, 사내 서버 어디든 쉽게 옮길 수 있음.<br>- **격리성**: 하나의 서비스가 죽어도 다른 서비스에 영향을 덜 줌. |
| **단점** | - **학습 비용**: Dockerfile, YAML 설정, 네트워크 구조 등을 배워야 함.<br>- **리소스 오버헤드**: 가상화 기술이므로 베어메탈(쌩 컴퓨터)보다는 성능이 약간 떨어질 수 있음. |

## 6. 🆚 차별점 (Differences)
- **vs 로컬 실행**:
  - 로컬 `python main.py`는 터미널 끄면 꺼지지만, Docker는 데몬 형태로 백그라운드에서 계속 돔.
- **vs Serverless (Lambda)**:
  - LLM은 메모리를 많이 먹고 실행 시간이 길어서(Timeout 이슈), 서버리스보다는 컨테이너 방식(ECS, K8s)이 유리함.

## 7. 🔗 관련 기술 (Related Tech)
- **FastAPI Uvicorn Workers**: 파이썬은 기본적으로 싱글 코어만 쓰지만, 워커를 늘려 멀티 코어를 활용하게 설정 가능.
- **Prometheus / Grafana**: 서버 상태와 매트릭(CPU, 요청 수)을 시각화하는 모니터링 도구.
- **Sentry**: 에러 발생 시 즉시 알림을 보내주는 버그 추적  시스템.

---

# 🎓 Q&A: 핵심 질문 리스트

이 문서는 RAG/LLM 시스템 구축 과정에서 자주 발생하는 질문과 기술 면접/학습 점검용 답변을 정리했습니다.

---

## 🏗️ PART 1~3: 인프라 및 데이터

### Q1. VectorDB는 RDB(SQL)와 무엇이 다른가요?
- **RDB**는 정확한 값(키워드, ID)을 매칭하는 데 특화되어 있고, **VectorDB**는 데이터의 **의미적 유사도(거리)**를 계산하는 데 특화되어 있습니다.
- 예를 들어, RDB에서 `SELECT * FROM docs WHERE content LIKE '%휴가%'`를 하면 "연차"라는 단어는 못 찾지만, VectorDB는 "휴가"와 "연차"의 벡터 거리가 가까우므로 찾아낼 수 있습니다.

### Q2. 임베딩(Embedding)이란 무엇이며 왜 필요한가요?
- 컴퓨터는 사람의 언어(한글/영어)를 이해하지 못합니다. 임베딩은 텍스트를 **실수형 벡터(숫자 리스트)**로 변환하여 컴퓨터가 계산할 수 있게 만드는 과정입니다.
- 의미가 비슷한 단어는 벡터 공간에서 가까운 위치에 좌표가 찍히게 됩니다.

### Q3. 청킹(Chunking) 사이즈는 어떻게 정해야 하나요?
- 정답은 없지만, 일반적으로 **500~1000자(Token 기준 256~512)** 정도가 적당합니다.
- 너무 작으면 문맥(Context)이 잘려서 이해하기 어렵고, 너무 크면 LLM의 Context Window를 많이 차지하고 검색 정밀도가 떨어집니다. 보통 **Overlap(중복 구간)**을 10~20% 두어 문맥 끊김을 방지합니다.

---

## 🧠 PART 4~5: RAG 및 에이전트

### Q4. RAG를 썼는데도 할루시네이션(거짓말)을 합니다. 왜 그런가요?
- **검색 실패**: 애초에 이상한 문서를 가져왔을 경우 (Garbage In).
- **LLM의 고집**: LLM이 검색된 문서보다 자기가 학습한 지식(Pre-trained knowledge)을 더 우선시할 경우.
- **해결책**: 프롬프트에 "반드시 주어진 문서에서만 답하라", "문서에 없으면 모른다고 하라"고 강력하게 지시하거나, **ReRanker**를 도입해 검색 품질을 높여야 합니다.

### Q5. Router/Agent는 언제 필요한가요?
- 사용자의 질문이 **복합적**일 때 필요합니다.
- 예: "지난달 A팀 매출(DB 조회)이랑 우리 회사 휴가 규정(문서 검색) 알려줘."
- 단순 RAG는 DB를 볼 수 없고, 단순 SQL 봇은 문서를 볼 수 없으므로, 질문 의도를 판단해 적절한 도구를 연결해주는 **Router**가 필수적입니다.

---

## 🔧 PART 6~7: 튜닝 및 최적화

### Q6. LangChain이 너무 복잡합니다. 꼭 써야 하나요?
- 필수는 아닙니다. 단순한 API 호출은 `requests`나 `openai` 패키지로 직접 짜는 게 더 빠를 수 있습니다.
- 하지만 문서 로더(PDF, Docx 등), 텍스트 분할기, VectorDB 연동, 메모리 관리 등 **공통 기능을 직접 구현하는 비용**이 크기 때문에, 복잡한 앱을 만들 때는 LangChain 같은 프레임워크가 장기적으로 유리합니다.

### Q7. ReRanker를 쓰면 속도가 느려지지 않나요?
- 네, 느려집니다. 1차 검색(Vector)은 0.01초 단위지만, ReRanker(Cross-Encoder)는 문서 하나하나를 모델에 통과시키므로 수십 배 느립니다.
- 따라서 **1차에서 50~100개를 빠르게 추리고(Recall)**, **2차에서 상위 5~10개만 정밀하게 재순위화(Precision)**하는 **2-Pass 전략**을 사용합니다.

### Q8. Hybrid Search(하이브리드 검색)는 무엇인가요?
- **Vector Search(의미)** + **Keyword Search(단어 매칭)**을 결합한 것입니다.
- "2024년" 같은 특정 연도나 "갤럭시 S24" 같은 특정 모델명은 키워드 검색이 정확하고, "배터리 오래 가는 폰 추천해줘" 같은 질문은 벡터 검색이 유리합니다. 이 둘의 장점을 합치는 것입니다.

---

## 🚀 PART 8: 운영

### Q9. 로컬 LLM을 서비스에 쓸 때 가장 큰 문제는 무엇인가요?
- **동시성(Concurrency)**입니다. ChatGPT API는 수천 명이 동시에 써도 되지만, 로컬 GPU/CPU는 한 번에 하나의 요청을 처리하는 데도 자원을 많이 씁니다.
- 사용자가 몰리면 대기 시간이 길어지므로, **vLLM** 같은 고성능 서빙 프레임워크를 쓰거나 큐(Queue) 시스템을 잘 설계해야 합니다.

### Q10. 데이터 보안 측면에서 로컬 RAG의 장점은?
- 사내의 민감한 데이터(연봉, 계약서 등)가 OpenAI나 Google 서버로 전송되지 않고, **물리적으로 차단된 사내 서버(On-Premise)** 안에서만 맴돌기 때문에 보안 규정이 엄격한 금융/공공/기업 환경에 적합합니다.
