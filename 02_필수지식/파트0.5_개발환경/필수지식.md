# PART 0.5. 개발 환경 설정

## 1. 탄생 배경 (Background)
- **로컬 LLM의 부상**: 클라우드 의존 없이 내 PC에서 LLM을 돌릴 수 있는 기술(Ollama, Llama.cpp)이 발전함에 따라, 개인/사내 서버에서도 AI 서비스 구축이 가능해짐.
- **하드웨어 제약**: 고가의 H100 GPU 없이도 **RAM 16GB** 수준의 일반 장비에서 돌아가는 효율적인 아키텍처가 필요.

## 2. 개념 (Concept)
- **로컬 호스팅 (Local Hosting)**: 외부 API(OpenAI 등)를 호출하지 않고, 내 컴퓨터의 연산 자원(CPU/GPU/RAM)을 사용하여 모델을 직접 실행하는 방식.
- **가상환경 (Virtual Environment)**: Python 프로젝트 간 라이브러리 충돌을 방지하기 위한 격리된 실행 환경.

## 3. 역할 (Role)
- **인프라 기반**: RAG 시스템이 구동될 물리적/소프트웨어적 토대 마련.
- **모델 서빙**: DeepSeek-R1(텍스트), LLaVA(비전) 등 AI 모델을 메모리에 로드하고 API로 제공.

## 4. 도입 이유 (Reason)
- **실습의 재현성**: 모든 독자가 동일한 환경(Python 버전, 라이브러리)에서 실패 없이 코드를 실행하기 위함.
- **비용 통제**: API 요금 걱정 없이 무제한으로 AI를 테스트하기 위함.

## 5. 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **데이터 주권**: 내 데이터가 내 PC를 떠나지 않음.<br>- **유지보수**: 인터넷이 끊겨도 서비스 가능. |
| **단점** | - **하드웨어 리소스**: RAM 16GB 이상 필요 (DeepSeek + LLaVA 동시 구동 시 부하).<br>- **초기 설정**: Docker, Python, Ollama 등 설치 과정이 번거로움. |

## 6. 차별점 (Differences)
로컬 환경과 클라우드 환경의 주요 차이점은 다음과 같습니다.

| 비교 항목 | 로컬 환경 (On-Premise) | 클라우드 서비스 (OpenAI/AWS) |
| :--- | :--- | :--- |
| **데이터 보안** | 외부 유출 위험 없음 (망 분리 가능) | 데이터가 외부 서버로 전송됨 |
| **비용** | 초기 장비 구매 외 무제한 무료 | 사용량에 비례하여 지속적 비용 발생 |
| **인터넷 연결** | 인터넷 없이도 사용 가능 (폐쇄망) | 반드시 인터넷 연결 필요 |
| **설정 복잡도** | 초기 설치 및 환경 설정이 다소 복잡함 | API 키 발급만으로 즉시 사용 가능 |
| **하드웨어 요구** | 고성능 CPU/GPU 및 RAM 16GB 이상 필요 | 저사양 PC에서도 브라우저만으로 충분 |

> **팁**: 만약 개인 PC 사양이 낮아 로컬 모델 구동이 어렵다면, **OpenAI API(유료 결제)**를 대안으로 추천합니다. 초기 학습 단계에서는 환경 설정의 부담 없이 강력한 AI의 성능을 즉시 경험해 볼 수 있습니다.

## 7. 관련 기술 (Related Tech)
- **Ollama**: 가장 쉬운 로컬 LLM 실행 도구. 모델 다운로드/실행을 CLI 한 줄로 처리.
- **Docker**: DB(PostgreSQL), VectorDB(Chroma) 등을 컨테이너로 띄워 환경 종속성 제거.
- **PostgreSQL**: 정형 데이터를 저장할 관계형 데이터베이스.
