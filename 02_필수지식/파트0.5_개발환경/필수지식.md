# PART 0.5. 개발 환경 설정

## 1. 📌 탄생 배경 (Background)
- **로컬 LLM의 부상**: 클라우드 의존 없이 내 PC에서 LLM을 돌릴 수 있는 기술(Ollama, Llama.cpp)이 발전함에 따라, 개인/사내 서버에서도 AI 서비스 구축이 가능해짐.
- **하드웨어 제약**: 고가의 H100 GPU 없이도 **RAM 16GB** 수준의 일반 장비에서 돌아가는 효율적인 아키텍처가 필요.

## 2. 🔑 개념 (Concept)
- **로컬 호스팅 (Local Hosting)**: 외부 API(OpenAI 등)를 호출하지 않고, 내 컴퓨터의 연산 자원(CPU/GPU/RAM)을 사용하여 모델을 직접 실행하는 방식.
- **가상환경 (Virtual Environment)**: Python 프로젝트 간 라이브러리 충돌을 방지하기 위한 격리된 실행 환경.

## 3. ⚙️ 역할 (Role)
- **인프라 기반**: RAG 시스템이 구동될 물리적/소프트웨어적 토대 마련.
- **모델 서빙**: DeepSeek-R1(텍스트), LLaVA(비전) 등 AI 모델을 메모리에 로드하고 API로 제공.

## 4. 💡 도입 이유 (Reason)
- **실습의 재현성**: 모든 독자가 동일한 환경(Python 버전, 라이브러리)에서 실패 없이 코드를 실행하기 위함.
- **비용 통제**: API 요금 걱정 없이 무제한으로 AI를 테스트하기 위함.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **데이터 주권**: 내 데이터가 내 PC를 떠나지 않음.<br>- **유지보수**: 인터넷이 끊겨도 서비스 가능. |
| **단점** | - **하드웨어 리소스**: RAM 16GB 이상 필요 (DeepSeek + LLaVA 동시 구동 시 부하).<br>- **초기 설정**: Docker, Python, Ollama 등 설치 과정이 번거로움. |

## 6. 🆚 차별점 (Differences)
- **vs 클라우드 환경 (Colab/AWS)**:
  - 클라우드는 설정이 편하지만 비용이 계속 발생하고 데이터 보안 설정이 복잡함.
  - 로컬 환경은 초기 설정만 넘기면 영구적으로 무료에 가까움.

## 7. 🔗 관련 기술 (Related Tech)
- **Ollama**: 가장 쉬운 로컬 LLM 실행 도구. 모델 다운로드/실행을 CLI 한 줄로 처리.
- **Docker**: DB(PostgreSQL), VectorDB(Chroma) 등을 컨테이너로 띄워 환경 종속성 제거.
- **PostgreSQL**: 정형 데이터를 저장할 관계형 데이터베이스.
