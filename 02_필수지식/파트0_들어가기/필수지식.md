# PART 0. 이 책의 목표와 핵심 아키텍처

## 1. 📌 탄생 배경 (Background)
- **문제점**: 기존 LLM(ChatGPT 등)은 범용적인 지식은 뛰어나지만, **"폐쇄된 사내 데이터"**를 알지 못함.
- **왜 지금 Fine-tuning이 아닌가?**:
  - 비용 문제보다는 **적합한 단계의 문제**임. 데이터가 수시로 바뀌는 환경(규정 변경, 신규 직원)에서 매번 재학습(Fine-tuning)을 하는 것은 비효율적.
  - 지금은 모델에게 "암기"를 시키기보다 "실시간으로 책을 찾아보는 법"을 가르치는 **RAG**가 필수적인 단계.
- **그럼 언제 Fine-tuning을 해야 하는가? (Next Level)**:
  - **도메인 특화**: 사내 약어, 은어, 전문 용어가 너무 많아 일반 모델이 문맥을 이해하지 못할 때.
  - **스타일 일치 (Tone & Manner)**: 회사의 보고서 양식이나 특유의 응답 스타일을 완벽하게 따라야 할 때.
  - **경량화**: 거대한 모델의 추론 능력을 작은 모델(SLM)에 이식하여 비용을 줄이고 싶을 때.
- **해법**: RAG와 MCP로 **"지식을 연결"**하는 시스템을 먼저 구축한 뒤, 필요에 따라 Fine-tuning으로 **"지능을 특화"**시키는 것이 올바른 순서.

## 2. 🔑 개념 (Concept)
- **RAG (Retrieval-Augmented Generation)**: LLM이 답변을 생성하기 전에, 외부 데이터베이스(VectorDB)에서 관련 정보를 **검색(Retrieval)**하여 참고 자료로 제공하는 기술.
- **MCP (Model Context Protocol)**: LLM이 데이터베이스(SQL)나 API 같은 외부 도구를 표준화된 방식으로 연결하여 사용할 수 있게 하는 **연결 프로토콜**.

## 3. ⚙️ 역할 (Role)
- **사내 지식 베이스**: 흩어진 문서(PDF, Word, HWP)를 검색 가능한 지식으로 변환.
- **업무 자동화**: 정형 데이터(매출, 연차) 조회와 비정형 데이터(규정) 검색을 통합하여 직원의 반복 질문 해결.

## 4. 💡 도입 이유 (Reason)
- **데이터 보안**: 로컬 LLM(Ollama)을 사용하여 데이터가 외부(OpenAI 등)로 유출되지 않음.
- **최신성 유지**: 모델 재학습 없이 문서만 업데이트하면 즉시 AI가 새로운 내용을 알게 됨.
- **비용 효율성**: 값비싼 GPU 클러스터 없이 16GB RAM 수준의 일반 워크스테이션에서 구축 가능.

## 5. ⚖️ 장단점 (Pros & Cons)
| 구분 | 내용 |
| :--- | :--- |
| **장점** | - **할루시네이션 감소**: 근거 문서를 보고 답하므로 거짓말이 줄어듦.<br>- **보안**: 사내망 내부에서 완결적인 시스템 구축 가능.<br>- **확장성**: 문서뿐만 아니라 DB, API 등 다양한 소스 연결 가능. |
| **단점** | - **초기 구축 복잡성**: VectorDB, 임베딩, 검색 파이프라인 구축 필요.<br>- **검색 품질 의존도**: 검색이 실패하면 답변도 실패함 (Garbage In, Garbage Out). |

## 6. 🆚 차별점 (Differences)
- **vs Fine-tuning**: 
  - Fine-tuning은 "시험 공부"를 다시 시키는 것(지식 내재화).
  - RAG는 "오픈북 테스트"를 보게 하는 것(참고 자료 활용). 사내 업무에는 RAG가 훨씬 효율적.
- **vs ChatGPT Enterprise**:
  - ChatGPT는 데이터가 외부로 전송될 수 있다는 우려가 존재하지만, 이 아키텍처는 **100% 로컬(On-Premise)** 구동이 가능.

## 7. 🔗 관련 기술 (Related Tech)
- **Ollama / DeepSeek-R1**: 로컬 구동 가능한 고성능 오픈소스 LLM.
- **LangChain**: LLM과 검색기, 도구를 연결하는 오케스트레이션 프레임워크.
- **ChromaDB**: 문서 임베딩을 저장하는 경량 벡터 데이터베이스.
