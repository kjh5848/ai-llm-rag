# 파트 0. 이 책의 목표와 핵심 아키텍처: 기술 상세 레퍼런스

이 문서는 실무 중심의 AI 시스템 구축을 위한 핵심 기술 배경과 아키텍처 설계를 다룹니다. 작가님이 책의 챕터를 구성할 때 논리적 근거로 활용할 수 있도록 전문적인 기술 분석을 제공합니다.

---

## 0.1 기술적 기원: 왜 이 아키텍처가 필요한가?
*   **LLM의 정보 격리성**: 범용 LLM은 학습 데이터 컷오프(Cut-off) 이후의 정보나 기업 내부의 비공개 데이터(PDF, DB)를 물리적으로 알 수 없습니다.
*   **데이터 휘발성 대응**: 규정 변경, 인사 이동 등 매일 변하는 비즈니스의 동적 데이터를 정적인 모델 가중치(Fine-tuning)에 담는 것은 유지보수 측면에서 불가능에 가깝습니다.
*   **보안과 지식 주권**: 기업의 핵심 자산인 문서를 외부 클라우드로 전송하지 않고도 지능형 서비스를 구현하고자 하는 수요가 급증함에 따라 'On-Premise AI' 아키텍처가 필수적이 되었습니다.

## 0.2 아키텍처 설계: 핵심 구조와 기술적 정의
*   **RAG (Retrieval-Augmented Generation)**: 생성 모델이 답변하기 전, 신뢰할 수 있는 외부 데이터소스에서 지식을 **검색(Retrieval)**하여 컨텍스트를 **보강(Augmentation)**한 뒤 답변을 **생성(Generation)**하는 기술적 프레임워크입니다.
*   **MCP (Model Context Protocol)**: AI 모델이 사내 시스템(SQL, API, 로컬 파일 서버)과 표준화된 방식으로 소통하기 위한 연결 규격입니다. RAG가 '읽기' 중심이라면 MCP는 '행동과 연동' 중심의 프로토콜입니다.
*   **Reasoning (추론 능력)**: 단순히 단어를 나열하는 것이 아니라, 검색된 정보들 사이의 충돌을 해석하고 복잡한 계산 과정을 단계별(Chain of Thought)로 수행하는 능력입니다.

## 0.3 작동 메커니즘: 시스템 내에서의 핵심 역할
*   **지식 엔진 (Knowledge Engine)**: 비정형 문서를 데이터화하여 AI의 장기 기억 장치로 변환합니다.
*   **의사결정 보조 (Decision Support)**: "규정상 이 결재가 가능한가?"와 같은 질문에 대해 규정 문서와 결재 DB를 실시간으로 대조하여 답변합니다.
*   **환각 방지 안전장치 (Anti-Hallucination)**: LLM이 모르는 내용을 지어내기보다, 주어진 문서 내에서만 사고하도록 논리적 제약을 겁니다.

## 0.4 비즈니스 전략: 도입이 필요한 실무적 근거
*   **비용 효율성 (Cost-Efficiency)**: 대규모 모델 재학습 없이 문서 업데이트만으로 시스템 사양을 유지할 수 있습니다.
*   **신뢰성 (Reliability)**: 답변과 함께 근거 문서의 출처(파일명, 페이지)를 제시할 수 있어 사용자가 AI를 믿고 업무에 활용할 수 있게 합니다.
*   **인프라 유연성**: 로컬 환경(Ollama)을 지원하므로 폐쇄망 내부 배포가 용이합니다.

## 0.5 트레이드오프: 기술적 강점과 시스템 제약 사항
*   **강점**
    *   **투명성**: AI가 왜 그렇게 답했는지 추적이 가능합니다.
    *   **유연성**: 파일 업로드만으로 지식 업데이트가 즉시 이루어집니다.
*   **단점**
    *   **검색 종속성**: 관련 문서를 찾지 못하면 LLM의 성능과 무관하게 오답을 냅니다.
    *   **설계 복합성**: 문서를 잘게 자르는 전략(Chunking)에 따라 품질이 크게 좌우됩니다.

## 0.6 비교 우위: 대안 기술과의 결정적 차이점
*   **vs Fine-tuning**: 
    *   Fine-tuning은 모델의 '말투'나 '특정 분야 전문성'을 높이지만, RAG는 '정확한 사실 정보'를 전달합니다.
    *   실무에서는 RAG로 지식을 공급하고, Fine-tuning으로 답변 형식을 다듬는 것이 최상의 조합입니다.
*   **vs SQL Chatbot**: 단순 SQL 챗봇은 문서(규정)를 이해하지 못하고, RAG만으로는 숫자 계산이 약합니다. 본 아키텍처는 이 둘을 통합한 하이브리드 지능을 지향합니다.

## 0.7 연관 생태계: 통합 기술 스택
*   **Vector Database (Chroma, Pinecone)**: 의미 기반 검색을 위한 고유 DB.
*   **Embedding Models (nomic-embed-text)**: 텍스트를 숫자로 변환하는 핵심 모델.
*   **Orchestration (LangChain, LlamaIndex)**: 전체 워크플로우를 제어하는 프레임워크.

---
**실습 연결 포인트**:
- 해당 아키텍처의 철학은 `01_basic_rag/tutorial_step1_fail.md`의 실패 사례 분석과 `step3_rag.py`의 구현 코드에서 확인하실 수 있습니다.
